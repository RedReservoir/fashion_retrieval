{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import datasets.deep_fashion\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import Resize, Lambda, Compose\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import utils\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "capbm_image_transform = torchvision.models.ResNet50_Weights.DEFAULT.transforms()\n",
    "capbm_image_transform.antialias = True\n",
    "\n",
    "capbm_dataset_dir = os.path.join(pathlib.Path.home(), \"data\", \"DeepFashion\", \"Category and Attribute Prediction Benchmark\")\n",
    "capbm_dataset = datasets.deep_fashion.CatAttrPredBM(capbm_dataset_dir, capbm_image_transform)\n",
    "\n",
    "capbm_train_dataset = Subset(capbm_dataset, capbm_dataset.get_split_mask_idxs(\"train\"))\n",
    "capbm_test_dataset = Subset(capbm_dataset, capbm_dataset.get_split_mask_idxs(\"test\"))\n",
    "capbm_val_dataset = Subset(capbm_dataset, capbm_dataset.get_split_mask_idxs(\"val\"))\n",
    "\n",
    "###\n",
    "\n",
    "ctsrbm_image_transform = torchvision.models.ResNet50_Weights.DEFAULT.transforms()\n",
    "ctsrbm_image_transform.antialias = True\n",
    "\n",
    "ctsrbm_dataset_dir = os.path.join(pathlib.Path.home(), \"data\", \"DeepFashion\", \"Consumer-to-shop Clothes Retrieval Benchmark\")\n",
    "ctsrbm_dataset = datasets.deep_fashion.ConsToShopClothRetrBM(ctsrbm_dataset_dir, ctsrbm_image_transform)\n",
    "\n",
    "ctsrbm_train_dataset = Subset(ctsrbm_dataset, ctsrbm_dataset.get_split_mask_idxs(\"train\"))\n",
    "ctsrbm_test_dataset = Subset(ctsrbm_dataset, ctsrbm_dataset.get_split_mask_idxs(\"test\"))\n",
    "ctsrbm_val_dataset = Subset(ctsrbm_dataset, ctsrbm_dataset.get_split_mask_idxs(\"val\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets.combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_dataset = datasets.combine.Fusion([\n",
    "    capbm_train_dataset,\n",
    "    ctsrbm_train_dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308054"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fused_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset idx: 1\n",
      "Retrieval\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "dataset_idx = fused_dataset[idx][0]\n",
    "\n",
    "print(\"Dataset idx: {:d}\".format(dataset_idx))\n",
    "\n",
    "if dataset_idx == 0:\n",
    "\n",
    "    print(\"Classification\")\n",
    "    print(fused_dataset[idx][1][0].shape)\n",
    "    print(fused_dataset[idx][1][1])\n",
    "\n",
    "if dataset_idx == 1:\n",
    "\n",
    "    print(\"Retrieval\")\n",
    "    print(fused_dataset[idx][1][0].shape)\n",
    "    print(fused_dataset[idx][1][1].shape)\n",
    "    print(fused_dataset[idx][1][2].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_items = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(original_batch):\n",
    "\n",
    "    new_batch = []\n",
    "\n",
    "    dataset_idxs = np.asarray([item[0] for item in original_batch])\n",
    "    unique_dataset_idxs = np.unique(dataset_idxs)\n",
    "\n",
    "    for dataset_idx in unique_dataset_idxs:\n",
    "\n",
    "        dataset_batch = []\n",
    "\n",
    "        dataset_items = [item[1] for item in original_batch if item[0] == dataset_idx]\n",
    "\n",
    "        for subitem_idx in range(len(dataset_items[0])):\n",
    "\n",
    "            dataset_subitems = [item[subitem_idx] for item in dataset_items]\n",
    "            if type(dataset_subitems[0]) == int:\n",
    "                dataset_subitems = list(map(torch.tensor, dataset_subitems))\n",
    "\n",
    "            dataset_batch.append(torch.stack(dataset_subitems))\n",
    "\n",
    "        new_batch.append((dataset_idx, dataset_batch))\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_data_loader = DataLoader(fused_dataset, batch_size=8, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n",
      "0\n",
      "torch.Size([5, 3, 224, 224])\n",
      "torch.Size([5])\n",
      "torch.Size([5, 1000])\n",
      "1\n",
      "torch.Size([3, 3, 224, 224])\n",
      "torch.Size([3, 3, 224, 224])\n",
      "torch.Size([3, 3, 224, 224])\n",
      "torch.Size([3, 303])\n"
     ]
    }
   ],
   "source": [
    "for batch in fused_data_loader:\n",
    "\n",
    "    print(type(batch))\n",
    "    print(len(batch))\n",
    "\n",
    "    print(batch[0][0])\n",
    "    print(batch[0][1][0].shape)\n",
    "    print(batch[0][1][1].shape)\n",
    "    print(batch[0][1][2].shape)\n",
    "\n",
    "    print(batch[1][0])\n",
    "    print(batch[1][1][0].shape)\n",
    "    print(batch[1][1][1].shape)\n",
    "    print(batch[1][1][2].shape)\n",
    "    print(batch[1][1][3].shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_items"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_batch = [\n",
    "    (0, capbm_train_dataset[0]),\n",
    "    (1, ctsrbm_train_dataset[0]),\n",
    "    (0, capbm_train_dataset[1]),\n",
    "    (0, capbm_train_dataset[2]),\n",
    "    (1, ctsrbm_train_dataset[1]),\n",
    "    (0, capbm_train_dataset[3]),\n",
    "    (0, capbm_train_dataset[4]),\n",
    "    (1, ctsrbm_train_dataset[2])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2929807/2388173343.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dataset_batch = [torch.stack([torch.tensor(item[idx]) for item in dataset_items]) for idx in range(len(dataset_items[0]))]\n"
     ]
    }
   ],
   "source": [
    "dataset_idxs = np.asarray([item[0] for item in original_batch])\n",
    "unique_dataset_idxs = np.unique(dataset_idxs)\n",
    "\n",
    "new_batch = []\n",
    "\n",
    "for dataset_idx in unique_dataset_idxs:\n",
    "\n",
    "    dataset_items = [item[1] for item in original_batch if item[0] == dataset_idx]\n",
    "\n",
    "    num_items = \n",
    "\n",
    "    dataset_batch = [torch.stack([torch.tensor(item[idx]) for item in dataset_items]) for idx in range(len(dataset_items[0]))]\n",
    "\n",
    "    new_batch.append((dataset_idx, dataset_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 224, 224])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([item[0] for item in dataset_items]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(capbm_train_dataset[0][0]) == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "649142fb4cdab8a2d2387ea4a1c8e262f08b2b20e4af0e114d36ea602bf8b868"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
