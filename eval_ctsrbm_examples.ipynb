{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import pathlib\n",
    "import pickle as pkl\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from datasets import deep_fashion_ctsrbm\n",
    "from arch import backbones, models, heads\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils.mem\n",
    "import utils.list\n",
    "import utils.train\n",
    "import utils.time\n",
    "import utils.log\n",
    "import utils.dict\n",
    "import utils.sig\n",
    "import utils.pkl\n",
    "import utils.chunk\n",
    "import utils.arr\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "\n",
    "import json\n",
    "import socket\n",
    "\n",
    "\n",
    "\n",
    "def print_tensor_info(tensor, name, logger):\n",
    "\n",
    "    logger.print(\n",
    "        \"{:s}:\".format(name),\n",
    "        tensor.shape,\n",
    "        tensor.dtype,\n",
    "        tensor.device,\n",
    "        utils.mem.sprint_fancy_num_bytes(utils.mem.get_num_bytes(tensor))\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# COMPONENT FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def create_backbone(backbone_class):\n",
    "\n",
    "    if backbone_class == \"ResNet50Backbone\":\n",
    "        backbone = backbones.ResNet50Backbone()\n",
    "    if backbone_class == \"EfficientNetB3Backbone\":\n",
    "        backbone = backbones.EfficientNetB3Backbone()\n",
    "    if backbone_class == \"EfficientNetB4Backbone\":\n",
    "        backbone = backbones.EfficientNetB4Backbone()\n",
    "    if backbone_class == \"EfficientNetB5Backbone\":\n",
    "        backbone = backbones.EfficientNetB5Backbone()\n",
    "    if backbone_class == \"ConvNeXtTinyBackbone\":\n",
    "        backbone = backbones.ConvNeXtTinyBackbone(contiguous_after_permute=True)\n",
    "\n",
    "    return backbone\n",
    "\n",
    "\n",
    "def load_experiment_checkpoint(\n",
    "        experiment_checkpoint_filename,\n",
    "        exp_params,\n",
    "        device\n",
    "        ):\n",
    "\n",
    "    # Load checkpoint\n",
    "\n",
    "    experiment_checkpoint = torch.load(experiment_checkpoint_filename)\n",
    "\n",
    "    # Backbone\n",
    "\n",
    "    backbone_class = exp_params[\"settings\"][\"backbone\"][\"class\"]\n",
    "\n",
    "    backbone = create_backbone(backbone_class).to(device)\n",
    "\n",
    "    backbone.load_state_dict(experiment_checkpoint[\"backbone_state_dict\"])\n",
    "\n",
    "    # Head\n",
    "\n",
    "    ret_head = heads.RetHead(backbone.out_shape, 1024).to(device)\n",
    "\n",
    "    ret_head.load_state_dict(experiment_checkpoint[\"ret_head_state_dict\"])\n",
    "\n",
    "    return (backbone, ret_head)\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# DATASET FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def create_backbone_transform(backbone_class):\n",
    "\n",
    "    if backbone_class == \"ResNet50Backbone\":\n",
    "        backbone_image_transform = torchvision.models.ResNet50_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"EfficientNetB3Backbone\":\n",
    "        backbone_image_transform = torchvision.models.EfficientNet_B3_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"EfficientNetB4Backbone\":\n",
    "        backbone_image_transform = torchvision.models.EfficientNet_B4_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"EfficientNetB5Backbone\":\n",
    "        backbone_image_transform = torchvision.models.EfficientNet_B5_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"ConvNeXtTinyBackbone\":\n",
    "        backbone_image_transform = torchvision.models.ConvNeXt_Tiny_Weights.DEFAULT.transforms()\n",
    "\n",
    "    return backbone_image_transform\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# JSON DATA FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def save_json_data(\n",
    "        json_data_filename,\n",
    "        json_data\n",
    "        ):\n",
    "\n",
    "    with open(json_data_filename, 'w') as json_data_file:\n",
    "        json.dump(json_data, json_data_file, indent=2)\n",
    "\n",
    "\n",
    "def load_json_data(\n",
    "        json_data_filename\n",
    "        ):\n",
    "\n",
    "    with open(json_data_filename, 'r') as json_data_file:\n",
    "        json_data = json.load(json_data_file)\n",
    "\n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# PERFORMANCE FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def compute_embeddings_and_item_ids(\n",
    "    ret_model,\n",
    "    image_loader,\n",
    "    device,\n",
    "    with_tqdm\n",
    "):\n",
    "\n",
    "    ret_model.eval()\n",
    "\n",
    "    # Embedding calculation\n",
    "\n",
    "    all_img_embs = torch.tensor([], dtype=float).to(device)\n",
    "    all_item_ids = torch.tensor([], dtype=int).to(device)\n",
    "\n",
    "    loader_gen = image_loader\n",
    "    if with_tqdm: loader_gen = tqdm(loader_gen)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in loader_gen:\n",
    "\n",
    "            # img_embs\n",
    "\n",
    "            imgs = batch[0].to(device)\n",
    "            img_embs = ret_model(imgs)\n",
    "            all_img_embs = torch.cat([all_img_embs, img_embs])\n",
    "\n",
    "            # item_ids\n",
    "\n",
    "            item_ids = batch[1].to(device)\n",
    "            all_item_ids = torch.cat([all_item_ids, item_ids])\n",
    "\n",
    "    return all_img_embs, all_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance_metrics(\n",
    "    shop_img_embs,\n",
    "    shop_item_ids,\n",
    "    cons_img_embs,\n",
    "    cons_item_ids,\n",
    "    k_values,\n",
    "    cons_imgs_chunk_size,\n",
    "    with_tqdm\n",
    "):\n",
    "\n",
    "    num_cons_imgs = cons_img_embs.shape[0]\n",
    "\n",
    "    avg_p_at_k_dict = {k: 0 for k in k_values}\n",
    "    avg_r_at_k_dict = {k: 0 for k in k_values}\n",
    "\n",
    "    # Precision and recall metrics\n",
    "\n",
    "    ## [i]: Number of shop items with the same item id as cons img i\n",
    "\n",
    "    cons_shop_item_id_counts = torch.empty_like(cons_item_ids)\n",
    "\n",
    "    for idx, cons_item_id in enumerate(cons_item_ids):\n",
    "        counts = torch.sum(torch.eq(shop_item_ids, cons_item_id))\n",
    "        cons_shop_item_id_counts[idx] = counts\n",
    "\n",
    "    cons_img_idxs_chunk_gen = utils.chunk.chunk_partition_size(np.arange(num_cons_imgs), cons_imgs_chunk_size)\n",
    "    if with_tqdm: cons_img_idxs_chunk_gen = tqdm(cons_img_idxs_chunk_gen)\n",
    "\n",
    "    for cons_img_idxs_chunk in cons_img_idxs_chunk_gen:\n",
    "\n",
    "        ## [i, j]: Distance from shop img i to cons img j\n",
    "\n",
    "        shop_to_cons_dists = torch.cdist(shop_img_embs, cons_img_embs[cons_img_idxs_chunk, :])\n",
    "\n",
    "        ## [:, i]: Ordered closest shop img idxs to cons img i\n",
    "\n",
    "        shop_to_cons_ordered_idxs = torch.argsort(shop_to_cons_dists, dim=0)\n",
    "\n",
    "        ## [:, i]: ordered closest shop img item ids to cons img i \n",
    "\n",
    "        shop_to_cons_nearest_item_ids = shop_item_ids[shop_to_cons_ordered_idxs]\n",
    "\n",
    "        ## [:, i]: True/False if, for each shop image, the cons img i is of the same item id\n",
    "\n",
    "        shop_to_cons_hits = torch.eq(shop_to_cons_nearest_item_ids, cons_item_ids[cons_img_idxs_chunk])\n",
    "\n",
    "        for k in k_values:\n",
    "\n",
    "            k_corr = shop_img_embs.shape[0] if k == -1 else k\n",
    "\n",
    "            ## [i]: Number of hits of cons img i (out of the k first)\n",
    "\n",
    "            shop_to_cons_hits_sum = torch.sum(shop_to_cons_hits[:k_corr, ], dim=0)\n",
    "\n",
    "            ## [i]: p/r_at_k of cons img i\n",
    "\n",
    "            p_at_k = shop_to_cons_hits_sum / k_corr\n",
    "            r_at_k = shop_to_cons_hits_sum / cons_shop_item_id_counts[cons_img_idxs_chunk]\n",
    "\n",
    "            ## Accumulate results\n",
    "\n",
    "            avg_p_at_k_dict[k] += torch.sum(p_at_k).item()\n",
    "            avg_r_at_k_dict[k] += torch.sum(r_at_k).item()\n",
    "\n",
    "    ## Average results\n",
    "    \n",
    "    for k in k_values:\n",
    "\n",
    "        avg_p_at_k_dict[k] /= num_cons_imgs\n",
    "        avg_r_at_k_dict[k] /= num_cons_imgs\n",
    "\n",
    "    # Composite metrics\n",
    "\n",
    "    avg_f1_at_k_dict = {k: 2 / ((1 / avg_p_at_k_dict[k]) + (1 / avg_r_at_k_dict[k])) for k in k_values}\n",
    "\n",
    "    return avg_p_at_k_dict, avg_r_at_k_dict, avg_f1_at_k_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_closest_idxs(\n",
    "    shop_img_embs,\n",
    "    shop_img_idxs,\n",
    "    shop_item_ids,\n",
    "    cons_img_embs,\n",
    "    cons_img_idxs,\n",
    "    cons_item_ids,\n",
    "    desired_cons_img_idxs,\n",
    "    num_desired_shop_imgs,\n",
    "    cons_imgs_chunk_size,\n",
    "    with_tqdm,\n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    shop_img_idxs = torch.tensor(shop_img_idxs).to(device)\n",
    "    shop_item_ids = shop_item_ids.to(device)\n",
    "    \n",
    "    # Result tensors\n",
    "\n",
    "    num_desired_cons_imgs = len(desired_cons_img_idxs)\n",
    "\n",
    "    ## [i, j]: j-th closest shop img idx to cons img with desired zidx i\n",
    "    ## [i, j]: j-th closest shop item id to cons img with desired zidx i\n",
    "    ## [i, j]: j-th closest shop dist to cons img with desired zidx i\n",
    "\n",
    "    shop_to_desired_cons_ordered_closest_img_idxs = torch.empty(size=(num_desired_cons_imgs, num_desired_shop_imgs), dtype=int).to(device)\n",
    "    shop_to_desired_cons_ordered_closest_item_ids = torch.empty(size=(num_desired_cons_imgs, num_desired_shop_imgs), dtype=int).to(device)\n",
    "    shop_to_desired_cons_ordered_closest_dists = torch.empty(size=(num_desired_cons_imgs, num_desired_shop_imgs), dtype=float).to(device)\n",
    "    \n",
    "    # Counts of correct shop images\n",
    "\n",
    "    ## [i]: Number of shop items with the same item id as cons img i\n",
    "\n",
    "    cons_shop_item_id_counts = torch.empty_like(cons_item_ids)\n",
    "\n",
    "    for idx, cons_item_id in enumerate(cons_item_ids):\n",
    "        counts = torch.sum(torch.eq(shop_item_ids, cons_item_id))\n",
    "        cons_shop_item_id_counts[idx] = counts\n",
    "\n",
    "    # Preparing desired_cons_img_zidxs chunks\n",
    "\n",
    "    desired_cons_img_zidxs = utils.arr.compute_zidxs(cons_img_idxs, desired_cons_img_idxs)\n",
    "    desired_cons_img_zzidxs = np.arange(num_desired_cons_imgs)\n",
    "    \n",
    "    desired_cons_img_zidxs_chunk_gen = utils.chunk.chunk_partition_size(desired_cons_img_zidxs, cons_imgs_chunk_size)\n",
    "    desired_cons_img_zzidxs_chunk_gen = utils.chunk.chunk_partition_size(desired_cons_img_zzidxs, cons_imgs_chunk_size)    \n",
    "    \n",
    "    chunk_idxs_gen = zip(desired_cons_img_zidxs_chunk_gen, desired_cons_img_zzidxs_chunk_gen)\n",
    "    if with_tqdm: chunk_idxs_gen = tqdm(chunk_idxs_gen)\n",
    "\n",
    "    for desired_cons_img_zidxs_chunk, desired_cons_img_zzidxs_chunk in chunk_idxs_gen:\n",
    "\n",
    "        ## [j, i]: Distance from shop img zidx j to cons img with zidx i\n",
    "\n",
    "        shop_to_cons_dists = torch.cdist(shop_img_embs, cons_img_embs[desired_cons_img_zidxs_chunk, :])\n",
    "\n",
    "        ## [:, i]: Ordered closest shop img zidxs to cons img with zidx i\n",
    "\n",
    "        shop_to_cons_ordered_closest_zidxs = torch.argsort(shop_to_cons_dists, dim=0).to(device)\n",
    "\n",
    "        ## [:, i]: Ordered closest shop img idxs to cons img with zidx i \n",
    "        ## [:, i]: Ordered closest shop item ids to cons img with zidx i \n",
    "        ## [:, i]: Ordered closest shop img distances to cons img with zidx i \n",
    "\n",
    "        shop_to_cons_ordered_closest_img_idxs = shop_img_idxs[shop_to_cons_ordered_closest_zidxs]\n",
    "        shop_to_cons_ordered_closest_item_ids = shop_item_ids[shop_to_cons_ordered_closest_zidxs]\n",
    "        shop_to_cons_ordered_closest_dists = torch.take_along_dim(shop_to_cons_dists, shop_to_cons_ordered_closest_zidxs, dim=0)\n",
    "\n",
    "        shop_to_desired_cons_ordered_closest_img_idxs[desired_cons_img_zzidxs_chunk, :] = torch.t(shop_to_cons_ordered_closest_img_idxs[:num_desired_shop_imgs, :])\n",
    "        shop_to_desired_cons_ordered_closest_item_ids[desired_cons_img_zzidxs_chunk, :] = torch.t(shop_to_cons_ordered_closest_item_ids[:num_desired_shop_imgs, :])\n",
    "        shop_to_desired_cons_ordered_closest_dists[desired_cons_img_zzidxs_chunk, :] = torch.t(shop_to_cons_ordered_closest_dists[:num_desired_shop_imgs, :])\n",
    "\n",
    "    # Indexing counts\n",
    "\n",
    "    cons_shop_item_id_counts = cons_shop_item_id_counts[desired_cons_img_zzidxs]\n",
    "\n",
    "    return (\n",
    "        shop_to_desired_cons_ordered_closest_img_idxs,\n",
    "        shop_to_desired_cons_ordered_closest_item_ids,\n",
    "        shop_to_desired_cons_ordered_closest_dists,\n",
    "        cons_shop_item_id_counts\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "datetime_now_str = datetime.now().strftime(\"%d-%m-%Y--%H:%M:%S\")\n",
    "\n",
    "\n",
    "####\n",
    "# COMMAND ARGUMENTS\n",
    "####\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"eval_params_filename\", help=\"filename of the evaluation params json file inside the \\\"eval_params\\\" directory\")\n",
    "parser.add_argument(\"exp_params_filename\", help=\"filename of the experiment params json file inside the \\\"exp_params\\\" directory\")\n",
    "parser.add_argument(\"--silent\", help=\"no terminal prints will be made\", action=\"store_true\")\n",
    "parser.add_argument(\"--notqdm\", help=\"no tqdm bars will be shown\", action=\"store_true\")\n",
    "\n",
    "command = \"python eval_ctsrbm_examples.py resnet_ret_train/test_50_000__eval_ctsrbm_examples.json resnet_ret_train/test_50_000__train_ret_DDP.json\"\n",
    "command_args = parser.parse_args(command.split()[2:])\n",
    "\n",
    "eval_params_filename = os.path.join(pathlib.Path.home(), \"fashion_retrieval\", \"exp_params\", command_args.eval_params_filename)\n",
    "exp_params_filename = os.path.join(pathlib.Path.home(), \"fashion_retrieval\", \"exp_params\", command_args.exp_params_filename)\n",
    "\n",
    "with_tqdm = not command_args.notqdm and not command_args.silent\n",
    "\n",
    "####\n",
    "# EVALUATION PREREQUISITES\n",
    "####\n",
    "\n",
    "\n",
    "# Read params\n",
    "\n",
    "eval_params = load_json_data(eval_params_filename)\n",
    "exp_params = load_json_data(exp_params_filename)\n",
    "\n",
    "# Experiment directory\n",
    "\n",
    "experiment_name__eval = eval_params[\"experiment_name\"]\n",
    "experiment_name__exp = exp_params[\"experiment_name\"]\n",
    "\n",
    "if experiment_name__eval != experiment_name__exp:\n",
    "    raise ValueError(\"Experiment names do not match\")\n",
    "\n",
    "\n",
    "####\n",
    "# PREPARE LOGGER\n",
    "####\n",
    "\n",
    "\n",
    "experiment_name = eval_params[\"experiment_name\"]\n",
    "experiment_dirname = os.path.join(pathlib.Path.home(), \"data\", \"fashion_retrieval\", experiment_name)\n",
    "\n",
    "log_filename = \"eval_ctsrbm_examples_logs.txt\"\n",
    "log_full_filename = os.path.join(experiment_dirname, log_filename)\n",
    "if os.path.exists(log_full_filename):\n",
    "    os.remove(log_full_filename)\n",
    "\n",
    "logger_streams = [log_full_filename]\n",
    "if not command_args.silent: logger_streams.append(sys.stdout)\n",
    "\n",
    "logger = utils.log.Logger(logger_streams)\n",
    "\n",
    "logger.print(\"Command arguments:\", \" \".join(sys.argv))\n",
    "\n",
    "\n",
    "####\n",
    "# PREPARE EVAL DATA\n",
    "####\n",
    "\n",
    "\n",
    "eval_data = {}\n",
    "\n",
    "eval_data[\"script_name\"] = \"eval_ctsrbm_examples.py\"\n",
    "eval_data[\"command_args\"] = sys.argv\n",
    "eval_data[\"experiment_name\"] = eval_params[\"experiment_name\"]\n",
    "eval_data[\"settings\"] = {}\n",
    "eval_data[\"results\"] = {}\n",
    "\n",
    "eval_data[\"settings\"][\"datasets\"] = [\n",
    "    \"DeepFashion Consumer-to-shop Clothes Retrieval Benchmark\"\n",
    "]\n",
    "\n",
    "eval_data[\"settings\"][\"model_checkpoint\"] = eval_params[\"settings\"][\"model_checkpoint\"]\n",
    "\n",
    "\n",
    "####\n",
    "# GPU INITIALIZATION\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Selecting CUDA devices\")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "device_idx = eval_params[\"settings\"][\"device_idx\"]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(idx) for idx in [device_idx]])\n",
    "\n",
    "device = torch.device(0)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "eval_data[\"settings\"][\"gpu_usage\"] = utils.mem.list_gpu_data([device_idx])\n",
    "eval_data[\"settings\"][\"hostname\"] = socket.gethostname()\n",
    "\n",
    "logger.print(\"Selected CUDA devices\")\n",
    "\n",
    "logger.print(\"Current memory usage:\")\n",
    "logger.print(utils.mem.sprint_memory_usage([eval_params[\"settings\"][\"device_idx\"]], num_spaces=2))\n",
    "\n",
    "\n",
    "####\n",
    "# DATA INITIALIZATION\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Initializing image loader dataset\")\n",
    "\n",
    "# Dataset initialization\n",
    "\n",
    "backbone_class = exp_params[\"settings\"][\"backbone\"][\"class\"]\n",
    "\n",
    "backbone_image_transform = create_backbone_transform(backbone_class)\n",
    "backbone_image_transform.antialias = True\n",
    "\n",
    "ctsrbm_dataset_dir = os.path.join(pathlib.Path.home(), \"data\", \"DeepFashion\", \"Consumer-to-shop Clothes Retrieval Benchmark\")\n",
    "\n",
    "ctsrbm_dataset = deep_fashion_ctsrbm.ConsToShopClothRetrBmkImageLoader(ctsrbm_dataset_dir, img_transform=backbone_image_transform)\n",
    "\n",
    "logger.print(\"Initialized image loader dataset\")\n",
    "\n",
    "\n",
    "####\n",
    "# MODEL INITIALIZATION\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Loading model from checkpoint\")\n",
    "\n",
    "# Load components\n",
    "\n",
    "experiment_checkpoint_filename = eval_params[\"settings\"][\"model_checkpoint\"]\n",
    "\n",
    "experiment_checkpoint_filename_full = os.path.join(\n",
    "    experiment_dirname, experiment_checkpoint_filename\n",
    ")\n",
    "\n",
    "backbone, ret_head =\\\n",
    "load_experiment_checkpoint(\n",
    "    experiment_checkpoint_filename_full,\n",
    "    exp_params,\n",
    "    device\n",
    ")\n",
    "\n",
    "logger.print(\"Loaded model from checkpoint\")\n",
    "\n",
    "\n",
    "# Build models\n",
    "\n",
    "ret_model = models.BackboneAndHead(backbone, ret_head).to(device)\n",
    "\n",
    "logger.print(\"Loaded model from checkpoint\")\n",
    "\n",
    "\n",
    "####\n",
    "# TRAIN PERFORMANCE METRICS\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Train split examples begin\")\n",
    "\n",
    "# Data loader initialization\n",
    "\n",
    "train_shop_img_idxs = ctsrbm_dataset.get_subset_indices(split=\"train\", domain=\"shop\")\n",
    "train_cons_img_idxs = ctsrbm_dataset.get_subset_indices(split=\"train\", domain=\"consumer\")\n",
    "\n",
    "train_shop_dataset = Subset(ctsrbm_dataset, train_shop_img_idxs)\n",
    "train_cons_dataset = Subset(ctsrbm_dataset, train_cons_img_idxs)\n",
    "\n",
    "batch_size = eval_params[\"settings\"][\"data_loading\"][\"batch_size\"]\n",
    "num_workers = eval_params[\"settings\"][\"data_loading\"][\"num_workers\"]\n",
    "\n",
    "train_shop_loader = DataLoader(\n",
    "    train_shop_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_cons_loader = DataLoader(\n",
    "    train_cons_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding calculation\n",
    "\n",
    "logger.print(\"  Computing image embeddings\")\n",
    "\n",
    "train_shop_img_embs, train_shop_item_ids = compute_embeddings_and_item_ids(\n",
    "    ret_model,\n",
    "    train_shop_loader,\n",
    "    device,\n",
    "    with_tqdm\n",
    "    \n",
    ")\n",
    "\n",
    "train_cons_img_embs, train_cons_item_ids = compute_embeddings_and_item_ids(\n",
    "    ret_model,\n",
    "    train_cons_loader,\n",
    "    device,\n",
    "    with_tqdm\n",
    ")\n",
    "\n",
    "logger.print(\"  Computed image embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example calculation\n",
    "\n",
    "logger.print(\"  Computing retrieval examples\")\n",
    "\n",
    "train_desired_cons_img_idxs = eval_params[\"settings\"][\"train\"][\"desired_cons_img_idxs\"]\n",
    "train_num_desired_shop_imgs = eval_params[\"settings\"][\"train\"][\"num_desired_shop_imgs\"]\n",
    "cons_imgs_chunk_size = utils.dict.chain_get(\n",
    "    eval_params,\n",
    "    \"settings\", \"cons_imgs_chunk_size\",\n",
    "    default=1000\n",
    ")\n",
    "\n",
    "(\n",
    "    shop_to_desired_cons_ordered_closest_img_idxs,\n",
    "    shop_to_desired_cons_ordered_closest_item_ids,\n",
    "    shop_to_desired_cons_ordered_closest_dists,\n",
    "    cons_shop_item_id_counts\n",
    ") = compute_closest_idxs(\n",
    "    train_shop_img_embs,\n",
    "    train_shop_img_idxs,\n",
    "    train_shop_item_ids,\n",
    "    train_cons_img_embs,\n",
    "    train_cons_img_idxs,\n",
    "    train_cons_item_ids,\n",
    "    train_desired_cons_img_idxs,\n",
    "    train_num_desired_shop_imgs,\n",
    "    cons_imgs_chunk_size,\n",
    "    with_tqdm,\n",
    "    device\n",
    ")\n",
    "\n",
    "logger.print(\"  Computed retrieval examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data\n",
    "\n",
    "eval_data[\"results\"][\"train\"] = {}\n",
    "\n",
    "train_desired_cons_img_zidxs = utils.arr.compute_zidxs(train_cons_img_idxs, train_desired_cons_img_idxs)\n",
    "\n",
    "for train_desired_cons_img_zidx, train_desired_cons_img_idx in enumerate(train_desired_cons_img_idxs):\n",
    "\n",
    "    cons_item_id = train_cons_item_ids[train_desired_cons_img_zidx]\n",
    "    shop_item_id_counts = cons_shop_item_id_counts[train_desired_cons_img_zidx]\n",
    "    closest_shop_img_idxs = shop_to_desired_cons_ordered_closest_img_idxs[train_desired_cons_img_zidx, :]\n",
    "    closest_shop_item_ids = shop_to_desired_cons_ordered_closest_item_ids[train_desired_cons_img_zidx, :]\n",
    "    closest_shop_img_dists = shop_to_desired_cons_ordered_closest_dists[train_desired_cons_img_zidx, :]\n",
    "\n",
    "    eval_data[\"results\"][\"train\"][train_desired_cons_img_idx] = {\n",
    "        \"cons_item_id\": cons_item_id.item(),\n",
    "        \"shop_item_id_counts\": shop_item_id_counts.item(),\n",
    "        \"closest_shop_img_idxs\": closest_shop_img_idxs.tolist(),\n",
    "        \"closest_shop_item_ids\": closest_shop_item_ids.tolist(),\n",
    "        \"closest_shop_img_dists\": closest_shop_img_dists.tolist()\n",
    "    }\n",
    "\n",
    "logger.print(\"  Current memory usage:\")\n",
    "logger.print(utils.mem.sprint_memory_usage([eval_params[\"settings\"][\"device_idx\"]], num_spaces=4))\n",
    "\n",
    "logger.print(\"Train split examples end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1d9cd8eb66510c5ec86eb907d6561b8001175da1689fbe0f45c40d854d32b14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
