{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import pathlib\n",
    "import pickle as pkl\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from datasets import deep_fashion_ctsrbm\n",
    "from arch import backbones, models, heads\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils.mem\n",
    "import utils.list\n",
    "import utils.train\n",
    "import utils.time\n",
    "import utils.log\n",
    "import utils.dict\n",
    "import utils.sig\n",
    "import utils.pkl\n",
    "import utils.chunk\n",
    "import utils.ten\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "\n",
    "import json\n",
    "import socket\n",
    "\n",
    "\n",
    "\n",
    "def print_tensor_info(tensor, name, logger):\n",
    "\n",
    "    logger.print(\n",
    "        \"{:s}:\".format(name),\n",
    "        tensor.shape,\n",
    "        tensor.dtype,\n",
    "        tensor.device,\n",
    "        utils.mem.sprint_fancy_num_bytes(utils.mem.get_num_bytes(tensor))\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# COMPONENT FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def create_backbone(backbone_class):\n",
    "\n",
    "    if backbone_class == \"ResNet50Backbone\":\n",
    "        backbone = backbones.ResNet50Backbone()\n",
    "    if backbone_class == \"EfficientNetB3Backbone\":\n",
    "        backbone = backbones.EfficientNetB3Backbone()\n",
    "    if backbone_class == \"EfficientNetB4Backbone\":\n",
    "        backbone = backbones.EfficientNetB4Backbone()\n",
    "    if backbone_class == \"EfficientNetB5Backbone\":\n",
    "        backbone = backbones.EfficientNetB5Backbone()\n",
    "    if backbone_class == \"ConvNeXtTinyBackbone\":\n",
    "        backbone = backbones.ConvNeXtTinyBackbone(contiguous_after_permute=True)\n",
    "\n",
    "    return backbone\n",
    "\n",
    "\n",
    "def load_experiment_checkpoint(\n",
    "        experiment_checkpoint_filename,\n",
    "        exp_params,\n",
    "        device\n",
    "        ):\n",
    "\n",
    "    # Load checkpoint\n",
    "\n",
    "    experiment_checkpoint = torch.load(experiment_checkpoint_filename)\n",
    "\n",
    "    # Backbone\n",
    "\n",
    "    backbone_class = exp_params[\"settings\"][\"backbone\"][\"class\"]\n",
    "\n",
    "    backbone = create_backbone(backbone_class).to(device)\n",
    "\n",
    "    backbone.load_state_dict(experiment_checkpoint[\"backbone_state_dict\"])\n",
    "\n",
    "    # Head\n",
    "\n",
    "    ret_head = heads.RetHead(backbone.out_shape, 1024).to(device)\n",
    "\n",
    "    ret_head.load_state_dict(experiment_checkpoint[\"ret_head_state_dict\"])\n",
    "\n",
    "    return (backbone, ret_head)\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# DATASET FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def create_backbone_transform(backbone_class):\n",
    "\n",
    "    if backbone_class == \"ResNet50Backbone\":\n",
    "        backbone_image_transform = torchvision.models.ResNet50_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"EfficientNetB3Backbone\":\n",
    "        backbone_image_transform = torchvision.models.EfficientNet_B3_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"EfficientNetB4Backbone\":\n",
    "        backbone_image_transform = torchvision.models.EfficientNet_B4_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"EfficientNetB5Backbone\":\n",
    "        backbone_image_transform = torchvision.models.EfficientNet_B5_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"ConvNeXtTinyBackbone\":\n",
    "        backbone_image_transform = torchvision.models.ConvNeXt_Tiny_Weights.DEFAULT.transforms()\n",
    "\n",
    "    return backbone_image_transform\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# JSON DATA FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def save_json_data(\n",
    "        json_data_filename,\n",
    "        json_data\n",
    "        ):\n",
    "\n",
    "    with open(json_data_filename, 'w') as json_data_file:\n",
    "        json.dump(json_data, json_data_file, indent=2)\n",
    "\n",
    "\n",
    "def load_json_data(\n",
    "        json_data_filename\n",
    "        ):\n",
    "\n",
    "    with open(json_data_filename, 'r') as json_data_file:\n",
    "        json_data = json.load(json_data_file)\n",
    "\n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# PERFORMANCE FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def compute_embeddings_and_item_ids(\n",
    "    ret_model,\n",
    "    image_loader,\n",
    "    with_tqdm=True\n",
    "):\n",
    "\n",
    "    ret_model.eval()\n",
    "\n",
    "    # Embedding calculation\n",
    "\n",
    "    all_img_embs = torch.tensor([], dtype=float).to(device)\n",
    "    all_item_ids = torch.tensor([], dtype=int).to(device)\n",
    "\n",
    "    loader_gen = image_loader\n",
    "    if with_tqdm: loader_gen = tqdm(loader_gen)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in loader_gen:\n",
    "\n",
    "            # img_embs\n",
    "\n",
    "            imgs = batch[0].to(device)\n",
    "            img_embs = ret_model(imgs)\n",
    "            all_img_embs = torch.cat([all_img_embs, img_embs])\n",
    "\n",
    "            # item_ids\n",
    "\n",
    "            item_ids = batch[1].to(device)\n",
    "            all_item_ids = torch.cat([all_item_ids, item_ids])\n",
    "\n",
    "    return all_img_embs, all_item_ids\n",
    "\n",
    "\n",
    "def compute_performance_metrics(\n",
    "    shop_img_embs,\n",
    "    shop_item_ids,\n",
    "    cons_img_embs,\n",
    "    cons_item_ids,\n",
    "    k_values,\n",
    "    cons_imgs_chunk_size,\n",
    "    with_tqdm,\n",
    "    device\n",
    "):\n",
    "\n",
    "    num_cons_imgs = cons_img_embs.shape[0]\n",
    "\n",
    "    avg_p_at_k_dict = {k: 0 for k in k_values}\n",
    "    avg_r_at_k_dict = {k: 0 for k in k_values}\n",
    "\n",
    "    # Precision and recall metrics\n",
    "\n",
    "    ## [i]: Number of shop items with the same item id as cons img i\n",
    "\n",
    "    cons_shop_item_id_counts = torch.empty_like(cons_item_ids)\n",
    "\n",
    "    for idx, cons_item_id in enumerate(cons_item_ids):\n",
    "        counts = torch.sum(torch.eq(shop_item_ids, cons_item_id))\n",
    "        cons_shop_item_id_counts[idx] = counts\n",
    "\n",
    "    cons_img_idxs_chunk_gen = utils.chunk.chunk_partition_size(np.arange(num_cons_imgs), cons_imgs_chunk_size)\n",
    "    if with_tqdm: cons_img_idxs_chunk_gen = tqdm(cons_img_idxs_chunk_gen)\n",
    "\n",
    "    for cons_img_idxs_chunk in cons_img_idxs_chunk_gen:\n",
    "\n",
    "        ## [i, j]: Distance from shop img i to cons img j\n",
    "\n",
    "        shop_to_cons_dists = torch.cdist(shop_img_embs, cons_img_embs[cons_img_idxs_chunk, :])\n",
    "\n",
    "        ## [:, i]: Ordered closest shop img idxs to cons img i\n",
    "\n",
    "        shop_to_cons_ordered_idxs = torch.argsort(shop_to_cons_dists, dim=0)\n",
    "\n",
    "        ## [:, i]: ordered closest shop img item ids to cons img i \n",
    "\n",
    "        shop_to_cons_nearest_item_ids = shop_item_ids[shop_to_cons_ordered_idxs]\n",
    "\n",
    "        ## [:, i]: True/False if, for each shop image, the cons img i is of the same item id\n",
    "\n",
    "        shop_to_cons_hits = torch.eq(shop_to_cons_nearest_item_ids, cons_item_ids[cons_img_idxs_chunk])\n",
    "\n",
    "        # Metric calculation for each k value\n",
    "\n",
    "        cons_shop_item_id_counts_chunk = cons_shop_item_id_counts[cons_img_idxs_chunk]\n",
    "\n",
    "        for k in k_values:\n",
    "\n",
    "            if k == \"same\":\n",
    "\n",
    "                ## [i]: Number of hits of cons img i (out of the k first)\n",
    "\n",
    "                shop_to_cons_hits_sum = torch.zeros((shop_to_cons_hits.shape[1]), dtype=int).to(device)\n",
    "\n",
    "                for cons_img_zidx in range(shop_to_cons_hits.shape[1]):\n",
    "\n",
    "                    num_shop_items = cons_shop_item_id_counts_chunk[cons_img_zidx]\n",
    "                    shop_to_cons_hits_sum[cons_img_zidx] = torch.sum(shop_to_cons_hits[:num_shop_items, cons_img_zidx]).item()\n",
    "\n",
    "                ## [i]: Retrieval accuracy of cons img i\n",
    "\n",
    "                acc = shop_to_cons_hits_sum / cons_shop_item_id_counts_chunk\n",
    "\n",
    "                ## Accumulate results\n",
    "\n",
    "                avg_p_at_k_dict[k] += torch.sum(acc).item()\n",
    "                avg_r_at_k_dict[k] += torch.sum(acc).item()\n",
    "\n",
    "            else:\n",
    "\n",
    "                if k == \"all\":\n",
    "                    k_corr =  shop_img_embs.shape[0]\n",
    "                else:\n",
    "                    k_corr = k\n",
    "\n",
    "                ## [i]: Number of hits of cons img i (out of the k first)\n",
    "\n",
    "                shop_to_cons_hits_sum = torch.sum(shop_to_cons_hits[:k_corr, :], dim=0)\n",
    "\n",
    "                ## [i]: p/r_at_k of cons img i\n",
    "\n",
    "                p_at_k = shop_to_cons_hits_sum / k_corr\n",
    "                r_at_k = shop_to_cons_hits_sum / cons_shop_item_id_counts_chunk\n",
    "\n",
    "                ## Accumulate results\n",
    "\n",
    "                avg_p_at_k_dict[k] += torch.sum(p_at_k).item()\n",
    "                avg_r_at_k_dict[k] += torch.sum(r_at_k).item()\n",
    "\n",
    "    ## Average results\n",
    "    \n",
    "    for k in k_values:\n",
    "\n",
    "        avg_p_at_k_dict[k] /= num_cons_imgs\n",
    "        avg_r_at_k_dict[k] /= num_cons_imgs\n",
    "\n",
    "    # Composite metrics\n",
    "\n",
    "    avg_f1_at_k_dict = {k: 2 / ((1 / avg_p_at_k_dict[k]) + (1 / avg_r_at_k_dict[k])) for k in k_values}\n",
    "\n",
    "    return avg_p_at_k_dict, avg_r_at_k_dict, avg_f1_at_k_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command arguments: /home-net/gortega/fashion_retrieval/.venv/lib/python3.8/site-packages/ipykernel_launcher.py --ip=127.0.0.1 --stdin=9027 --control=9025 --hb=9024 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"1eb63fbb-ea33-467f-b3f8-ea83c3f95f86\" --shell=9026 --transport=\"tcp\" --iopub=9028 --f=/home-net/gortega/.local/share/jupyter/runtime/kernel-v2-2886418A8JKpb3QTqhZ.json\n",
      "Selecting CUDA devices\n",
      "Selected CUDA devices\n",
      "Current memory usage:\n",
      "  Device ID  0:  95.875 MiB /  11.000 GiB (  0.85%) - NVIDIA GeForce GTX 1080 Ti\n",
      "Initializing image loader dataset\n",
      "Initialized image loader dataset\n",
      "Loading model from checkpoint\n",
      "Loaded model from checkpoint\n",
      "Loaded model from checkpoint\n",
      "Train split metrics begin\n",
      "  Computing image embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 711/711 [00:40<00:00, 17.70it/s]\n",
      "100%|██████████| 3069/3069 [02:57<00:00, 17.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Computed image embeddings\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "datetime_now_str = datetime.now().strftime(\"%d-%m-%Y--%H:%M:%S\")\n",
    "\n",
    "\n",
    "####\n",
    "# COMMAND ARGUMENTS\n",
    "####\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"eval_params_filename\", help=\"filename of the evaluation params json file inside the \\\"eval_params\\\" directory\")\n",
    "parser.add_argument(\"exp_params_filename\", help=\"filename of the experiment params json file inside the \\\"exp_params\\\" directory\")\n",
    "parser.add_argument(\"--silent\", help=\"no terminal prints will be made\", action=\"store_true\")\n",
    "parser.add_argument(\"--notqdm\", help=\"no tqdm bars will be shown\", action=\"store_true\")\n",
    "\n",
    "command = \"python eval_ctsrbm_metrics.py resnet_ret_train/test_50_000__eval_ctsrbm_metrics.json resnet_ret_train/test_50_000__train_ret_DDP.json\"\n",
    "command_args = parser.parse_args(command.split()[2:])\n",
    "\n",
    "eval_params_filename = os.path.join(pathlib.Path.home(), \"fashion_retrieval\", \"exp_params\", command_args.eval_params_filename)\n",
    "exp_params_filename = os.path.join(pathlib.Path.home(), \"fashion_retrieval\", \"exp_params\", command_args.exp_params_filename)\n",
    "\n",
    "with_tqdm = not command_args.notqdm and not command_args.silent\n",
    "\n",
    "####\n",
    "# EVALUATION PREREQUISITES\n",
    "####\n",
    "\n",
    "\n",
    "# Read params\n",
    "\n",
    "eval_params = load_json_data(eval_params_filename)\n",
    "exp_params = load_json_data(exp_params_filename)\n",
    "\n",
    "# Experiment directory\n",
    "\n",
    "experiment_name__eval = eval_params[\"experiment_name\"]\n",
    "experiment_name__exp = exp_params[\"experiment_name\"]\n",
    "\n",
    "if experiment_name__eval != experiment_name__exp:\n",
    "    raise ValueError(\"Experiment names do not match\")\n",
    "\n",
    "\n",
    "####\n",
    "# PREPARE LOGGER\n",
    "####\n",
    "\n",
    "\n",
    "experiment_name = eval_params[\"experiment_name\"]\n",
    "experiment_dirname = os.path.join(pathlib.Path.home(), \"data\", \"fashion_retrieval\", experiment_name)\n",
    "\n",
    "log_filename = \"ctsrbm_eval_logs.txt\"\n",
    "log_full_filename = os.path.join(experiment_dirname, log_filename)\n",
    "if os.path.exists(log_full_filename):\n",
    "    os.remove(log_full_filename)\n",
    "\n",
    "#logger_streams = [log_full_filename]\n",
    "logger_streams = []\n",
    "if not command_args.silent: logger_streams.append(sys.stdout)\n",
    "\n",
    "logger = utils.log.Logger(logger_streams)\n",
    "\n",
    "logger.print(\"Command arguments:\", \" \".join(sys.argv))\n",
    "\n",
    "\n",
    "####\n",
    "# PREPARE EVAL DATA\n",
    "####\n",
    "\n",
    "\n",
    "eval_data = {}\n",
    "\n",
    "eval_data[\"experiment_name\"] = eval_params[\"experiment_name\"]\n",
    "eval_data[\"settings\"] = {}\n",
    "eval_data[\"results\"] = {}\n",
    "\n",
    "eval_data[\"settings\"][\"datasets\"] = [\n",
    "    \"DeepFashion Consumer-to-shop Clothes Retrieval Benchmark\"\n",
    "]\n",
    "\n",
    "eval_data[\"settings\"][\"model_checkpoint\"] = eval_params[\"settings\"][\"model_checkpoint\"]\n",
    "\n",
    "\n",
    "####\n",
    "# GPU INITIALIZATION\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Selecting CUDA devices\")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "device_idx = eval_params[\"settings\"][\"device_idx\"]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(idx) for idx in [device_idx]])\n",
    "\n",
    "device = torch.device(0)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "eval_data[\"settings\"][\"gpu_usage\"] = utils.mem.list_gpu_data([device_idx])\n",
    "eval_data[\"settings\"][\"hostname\"] = socket.gethostname()\n",
    "\n",
    "logger.print(\"Selected CUDA devices\")\n",
    "\n",
    "logger.print(\"Current memory usage:\")\n",
    "logger.print(utils.mem.sprint_memory_usage([eval_params[\"settings\"][\"device_idx\"]], num_spaces=2))\n",
    "\n",
    "\n",
    "####\n",
    "# DATA INITIALIZATION\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Initializing image loader dataset\")\n",
    "\n",
    "# Dataset initialization\n",
    "\n",
    "backbone_class = exp_params[\"settings\"][\"backbone\"][\"class\"]\n",
    "\n",
    "backbone_image_transform = create_backbone_transform(backbone_class)\n",
    "backbone_image_transform.antialias = True\n",
    "\n",
    "ctsrbm_dataset_dir = os.path.join(pathlib.Path.home(), \"data\", \"DeepFashion\", \"Consumer-to-shop Clothes Retrieval Benchmark\")\n",
    "\n",
    "ctsrbm_dataset = deep_fashion_ctsrbm.ConsToShopClothRetrBmkImageLoader(ctsrbm_dataset_dir, img_transform=backbone_image_transform)\n",
    "\n",
    "logger.print(\"Initialized image loader dataset\")\n",
    "\n",
    "\n",
    "####\n",
    "# MODEL INITIALIZATION\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Loading model from checkpoint\")\n",
    "\n",
    "# Load components\n",
    "\n",
    "experiment_checkpoint_filename = eval_params[\"settings\"][\"model_checkpoint\"]\n",
    "\n",
    "experiment_checkpoint_filename_full = os.path.join(\n",
    "    experiment_dirname, experiment_checkpoint_filename\n",
    ")\n",
    "\n",
    "backbone, ret_head =\\\n",
    "load_experiment_checkpoint(\n",
    "    experiment_checkpoint_filename_full,\n",
    "    exp_params,\n",
    "    device\n",
    ")\n",
    "\n",
    "logger.print(\"Loaded model from checkpoint\")\n",
    "\n",
    "\n",
    "# Build models\n",
    "\n",
    "ret_model = models.BackboneAndHead(backbone, ret_head).to(device)\n",
    "\n",
    "logger.print(\"Loaded model from checkpoint\")\n",
    "\n",
    "\n",
    "####\n",
    "# TRAIN PERFORMANCE METRICS\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Train split metrics begin\")\n",
    "\n",
    "# Data loader initialization\n",
    "\n",
    "train_shop_idxs = ctsrbm_dataset.get_subset_indices(split=\"train\", domain=\"shop\")\n",
    "train_cons_idxs = ctsrbm_dataset.get_subset_indices(split=\"train\", domain=\"consumer\")\n",
    "\n",
    "train_shop_dataset = Subset(ctsrbm_dataset, train_shop_idxs)\n",
    "train_cons_dataset = Subset(ctsrbm_dataset, train_cons_idxs)\n",
    "\n",
    "batch_size = eval_params[\"settings\"][\"data_loading\"][\"batch_size\"]\n",
    "num_workers = eval_params[\"settings\"][\"data_loading\"][\"num_workers\"]\n",
    "\n",
    "train_shop_loader = DataLoader(\n",
    "    train_shop_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_cons_loader = DataLoader(\n",
    "    train_cons_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Embedding calculation\n",
    "\n",
    "logger.print(\"  Computing image embeddings\")\n",
    "\n",
    "train_shop_img_embs, train_shop_item_ids =\\\n",
    "    compute_embeddings_and_item_ids(ret_model, train_shop_loader, with_tqdm)\n",
    "\n",
    "train_cons_img_embs, train_cons_item_ids =\\\n",
    "    compute_embeddings_and_item_ids(ret_model, train_cons_loader, with_tqdm)\n",
    "\n",
    "logger.print(\"  Computed image embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Computing performance metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:26<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Computed performance metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Metric calculation\n",
    "\n",
    "logger.print(\"  Computing performance metrics\")\n",
    "\n",
    "k_values = eval_params[\"settings\"][\"k_values\"]\n",
    "cons_imgs_chunk_size = utils.dict.chain_get(\n",
    "    eval_params,\n",
    "    \"settings\", \"cons_imgs_chunk_size\",\n",
    "    default=1000\n",
    ")\n",
    "\n",
    "avg_p_at_k_dict, avg_r_at_k_dict, avg_f1_at_k_dict =\\\n",
    "    compute_performance_metrics(\n",
    "        train_shop_img_embs,\n",
    "        train_shop_item_ids,\n",
    "        train_cons_img_embs,\n",
    "        train_cons_item_ids,\n",
    "        k_values,\n",
    "        cons_imgs_chunk_size,\n",
    "        with_tqdm,\n",
    "        device\n",
    "    )\n",
    "\n",
    "logger.print(\"  Computed performance metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Results:\n",
      "  Avg prec@k:\n",
      "  {'same': 0.03743232075859537, 'all': 7.261337974077232e-05, 1: 0.04401042727383813, 3: 0.025874710661792682, 5: 0.01944320013768697, 10: 0.013266262340572845}\n",
      "  Avg rec@k:\n",
      "  {'same': 0.03743232075859537, 'all': 1.0, 1: 0.03186369303515727, 3: 0.0550794256888632, 5: 0.06865161062767877, 10: 0.09296348667801249}\n",
      "  Avg f1@k:\n",
      "  {'same': 0.03743232075859537, 'all': 0.00014521621484139184, 1: 0.03696477110479092, 3: 0.03520917564839168, 5: 0.030303873601901014, 10: 0.023219070246491787}\n",
      "  Current memory usage:\n",
      "    Device ID  0:   8.900 GiB /  11.000 GiB ( 80.91%) - NVIDIA GeForce GTX 1080 Ti\n",
      "Train split metrics end\n"
     ]
    }
   ],
   "source": [
    "# Save evaluation data\n",
    "\n",
    "eval_data[\"results\"][\"train\"] = {\n",
    "    \"avg_p_at_k_dict\": avg_p_at_k_dict,\n",
    "    \"avg_r_at_k_dict\": avg_r_at_k_dict,\n",
    "    \"avg_f1_at_k_dict\": avg_f1_at_k_dict\n",
    "}\n",
    "\n",
    "logger.print(\"  Results:\")\n",
    "logger.print(\"  Avg prec@k:\")\n",
    "logger.print(\"  \", avg_p_at_k_dict, sep=\"\")\n",
    "logger.print(\"  Avg rec@k:\")\n",
    "logger.print(\"  \", avg_r_at_k_dict, sep=\"\")\n",
    "logger.print(\"  Avg f1@k:\")\n",
    "logger.print(\"  \", avg_f1_at_k_dict, sep=\"\")\n",
    "\n",
    "logger.print(\"  Current memory usage:\")\n",
    "logger.print(utils.mem.sprint_memory_usage([eval_params[\"settings\"][\"device_idx\"]], num_spaces=4))\n",
    "\n",
    "logger.print(\"Train split metrics end\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1d9cd8eb66510c5ec86eb907d6561b8001175da1689fbe0f45c40d854d32b14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
