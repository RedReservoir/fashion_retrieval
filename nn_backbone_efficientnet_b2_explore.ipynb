{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "from fir.datasets import deep_fashion_ctsrbm\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.efficientnet_b2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"misc/efficientnet_b2_model_arch.txt\", \"w\") as out_file:\n",
    "    print(model, file=out_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.efficientnet_b2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctsrbm_image_transform = torchvision.models.EfficientNet_B2_Weights.DEFAULT.transforms()\n",
    "ctsrbm_image_transform.antialias = True\n",
    "ctsrbm_dataset_dir = os.path.join(pathlib.Path.home(), \"data\", \"DeepFashion\", \"Consumer-to-shop Clothes Retrieval Benchmark\")\n",
    "ctsrbm_dataset = deep_fashion_ctsrbm.ConsToShopClothRetrBmkImageLoader(ctsrbm_dataset_dir, ctsrbm_image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[288]\n",
       "    resize_size=[288]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctsrbm_image_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ctsrbm_dataset[0]\n",
    "\n",
    "img_tensor = input[0]\n",
    "item_id = input[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 288, 288])\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24444"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f9ea291ce20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.features.register_forward_hook(get_activation('features'))\n",
    "model.features[7][1].register_forward_hook(get_activation('features[7][2]'))\n",
    "model.features[8][1].register_forward_hook(get_activation('features[8][2]'))\n",
    "model.avgpool.register_forward_hook(get_activation('avgpool'))\n",
    "model.classifier.register_forward_hook(get_activation('classifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ctsrbm_dataset[0][0][None, :]\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input                torch.Size([1, 3, 288, 288])\n",
      "output               torch.Size([1, 1000])\n",
      "---\n",
      "features[7][2]       torch.Size([1, 352, 9, 9])\n",
      "features[8][2]       torch.Size([1, 1408, 9, 9])\n",
      "features             torch.Size([1, 1408, 9, 9])\n",
      "avgpool              torch.Size([1, 1408, 1, 1])\n",
      "classifier           torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(\"{:20s} {:}\".format(\"input\", input.shape))\n",
    "print(\"{:20s} {:}\".format(\"output\", output.shape))\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for key, item in activation.items():\n",
    "    print(\"{:20s} {:}\".format(key, item.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.efficientnet_b2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctsrbm_image_transform = torchvision.models.EfficientNet_B2_Weights.DEFAULT.transforms()\n",
    "ctsrbm_image_transform.antialias = True\n",
    "ctsrbm_image_transform.crop_size = [576]\n",
    "ctsrbm_image_transform.resize_size = [576]\n",
    "ctsrbm_dataset_dir = os.path.join(pathlib.Path.home(), \"data\", \"DeepFashion\", \"Consumer-to-shop Clothes Retrieval Benchmark\")\n",
    "ctsrbm_dataset = datasets.deep_fashion_ctsrbm.ConsToShopClothRetrBmkImageLoader(ctsrbm_dataset_dir, ctsrbm_image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[576]\n",
       "    resize_size=[576]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctsrbm_image_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fc5c6c41130>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.features.register_forward_hook(get_activation('features'))\n",
    "model.features[7][1].register_forward_hook(get_activation('features[7][2]'))\n",
    "model.features[8][1].register_forward_hook(get_activation('features[8][2]'))\n",
    "model.avgpool.register_forward_hook(get_activation('avgpool'))\n",
    "model.classifier.register_forward_hook(get_activation('classifier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ctsrbm_dataset[0][0][None, :]\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input                torch.Size([1, 3, 576, 576])\n",
      "output               torch.Size([1, 1000])\n",
      "---\n",
      "features[7][2]       torch.Size([1, 352, 18, 18])\n",
      "features[8][2]       torch.Size([1, 1408, 18, 18])\n",
      "features             torch.Size([1, 1408, 18, 18])\n",
      "avgpool              torch.Size([1, 1408, 1, 1])\n",
      "classifier           torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "print(\"{:20s} {:}\".format(\"input\", input.shape))\n",
    "print(\"{:20s} {:}\".format(\"output\", output.shape))\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for key, item in activation.items():\n",
    "    print(\"{:20s} {:}\".format(key, item.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.efficientnet_b2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctsrbm_image_transform = torchvision.models.EfficientNet_B2_Weights.DEFAULT.transforms()\n",
    "ctsrbm_image_transform.antialias = True\n",
    "ctsrbm_image_transform.crop_size = [576]\n",
    "ctsrbm_image_transform.resize_size = [576]\n",
    "ctsrbm_dataset_dir = os.path.join(pathlib.Path.home(), \"data\", \"DeepFashion\", \"Consumer-to-shop Clothes Retrieval Benchmark\")\n",
    "ctsrbm_dataset = datasets.deep_fashion_ctsrbm.ConsToShopClothRetrBmkImageLoader(ctsrbm_dataset_dir, ctsrbm_image_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[576]\n",
       "    resize_size=[576]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctsrbm_image_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fc5c6f72dc0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.features[8][1].register_forward_hook(get_activation('features[8][2]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 2\n",
      "65 3\n",
      "97 4\n",
      "129 5\n",
      "161 6\n",
      "193 7\n",
      "225 8\n",
      "257 9\n",
      "289 10\n",
      "321 11\n",
      "353 12\n",
      "385 13\n",
      "417 14\n",
      "449 15\n",
      "481 16\n",
      "513 17\n",
      "545 18\n"
     ]
    }
   ],
   "source": [
    "last_perm_size = 0\n",
    "\n",
    "for size in range(33, 600):\n",
    "\n",
    "    input = ctsrbm_dataset[0][0][None, :, :size, :size]\n",
    "    output = model(input)\n",
    "\n",
    "    for key, item in activation.items():\n",
    "        if key == \"features[8][2]\":\n",
    "            perm_size = item.size(3)\n",
    "\n",
    "    if last_perm_size < perm_size:\n",
    "        print(size, perm_size)\n",
    "        last_perm_size = perm_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in range(33, 576 + 1):\n",
    "\n",
    "    input = ctsrbm_dataset[0][0][None, :, :size, :size]\n",
    "    output = model(input)\n",
    "\n",
    "    for key, item in activation.items():\n",
    "        if key == \"features[8][2]\":\n",
    "            perm_size = item.size(3)\n",
    "\n",
    "    theo_perm_size = ((size - 1) // 32) + 1\n",
    "\n",
    "    if perm_size != theo_perm_size:\n",
    "        print(size, perm_size, theo_perm_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "649142fb4cdab8a2d2387ea4a1c8e262f08b2b20e4af0e114d36ea602bf8b868"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
