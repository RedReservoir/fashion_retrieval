{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pprint\n",
    "import json\n",
    "import itertools\n",
    "import shutil\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_dict(\n",
    "        json_filename,\n",
    "        json_dict\n",
    "        ):\n",
    "\n",
    "    with open(json_filename, 'w') as json_file:\n",
    "        json.dump(json_dict, json_file, indent=4)\n",
    "\n",
    "\n",
    "def load_json_dict(\n",
    "        json_filename\n",
    "        ):\n",
    "\n",
    "    with open(json_filename, 'r') as json_file:\n",
    "        json_dict = json.load(json_file)\n",
    "\n",
    "    return json_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_len = 98832\n",
    "val_dataset_len = 48935"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(pathlib.Path.home(), \"data\", \"fashion_retrieval\", \"ret_train\", \"convnext_t\")\n",
    "\n",
    "for data_subdir in os.listdir(data_dir):\n",
    "\n",
    "    data_filename = os.path.join(data_dir, data_subdir, \"exp_data.json\")\n",
    "    corr_data_filename = os.path.join(data_dir, data_subdir, \"exp_data_corr.json\")\n",
    "    \n",
    "    shutil.copyfile(data_filename, corr_data_filename)\n",
    "\n",
    "#\n",
    "\n",
    "data_dir = os.path.join(pathlib.Path.home(), \"data\", \"fashion_retrieval\", \"ret_train\", \"resnet_50\")\n",
    "\n",
    "for data_subdir in os.listdir(data_dir):\n",
    "\n",
    "    data_filename = os.path.join(data_dir, data_subdir, \"exp_data.json\")\n",
    "    corr_data_filename = os.path.join(data_dir, data_subdir, \"exp_data_corr.json\")\n",
    "    \n",
    "    shutil.copyfile(data_filename, corr_data_filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home-net/gortega/data/fashion_retrieval/ret_train/resnet_50/test_006/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/resnet_50/test_003/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/resnet_50/test_002/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/resnet_50/test_005/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/resnet_50/test_007/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/resnet_50/test_004/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/resnet_50/test_000/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/resnet_50/test_001/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/resnet_50/test_008/exp_data_corr.json\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(pathlib.Path.home(), \"data\", \"fashion_retrieval\", \"ret_train\", \"resnet_50\")\n",
    "\n",
    "for data_subdir in os.listdir(data_dir):\n",
    "\n",
    "    data_filename = os.path.join(data_dir, data_subdir, \"exp_data_corr.json\")\n",
    "    print(data_filename)\n",
    "\n",
    "    data = load_json_dict(data_filename)\n",
    "\n",
    "    #\n",
    "\n",
    "    num_gpus = len(data[\"settings\"][\"gpu_usage\"])\n",
    "\n",
    "    batch_size_1 = data[\"settings\"][\"stage_1\"][\"data_loading\"][\"batch_size\"]\n",
    "    grad_acc_iters_1 = data[\"settings\"][\"stage_1\"].get(\"max_acc_iter\", 1)\n",
    "    corr_lr_1 = data[\"settings\"][\"stage_1\"][\"learning_rate_list\"][0]\n",
    "\n",
    "    batch_size_2 = data[\"settings\"][\"stage_2\"][\"data_loading\"][\"batch_size\"]\n",
    "    grad_acc_iters_2 = data[\"settings\"][\"stage_2\"].get(\"max_acc_iter\", 1)\n",
    "    corr_lr_2 = data[\"settings\"][\"stage_2\"][\"learning_rate_list\"][0]\n",
    "\n",
    "    actual_batch_size_1 = batch_size_1 * grad_acc_iters_1 * num_gpus\n",
    "    actual_batch_size_2 = batch_size_2 * grad_acc_iters_2 * num_gpus\n",
    "\n",
    "    base_lr_1 = corr_lr_1 / actual_batch_size_1\n",
    "    base_lr_2 = corr_lr_2 / actual_batch_size_2\n",
    "\n",
    "    #\n",
    "\n",
    "    if \"max_acc_iter\" in data[\"settings\"][\"stage_1\"].keys():\n",
    "        del data[\"settings\"][\"stage_1\"][\"max_acc_iter\"]\n",
    "        data[\"settings\"][\"stage_1\"][\"data_loading\"][\"grad_acc_iters\"] = grad_acc_iters_1\n",
    "\n",
    "    if \"max_acc_iter\" in data[\"settings\"][\"stage_2\"].keys():\n",
    "        del data[\"settings\"][\"stage_2\"][\"max_acc_iter\"]\n",
    "        data[\"settings\"][\"stage_2\"][\"data_loading\"][\"grad_acc_iters\"] = grad_acc_iters_2\n",
    "\n",
    "    data[\"settings\"][\"stage_1\"][\"base_lr\"] = base_lr_1\n",
    "    data[\"settings\"][\"stage_2\"][\"base_lr\"] = base_lr_2\n",
    "\n",
    "    for idx in range(len(data[\"results\"][\"stage_1\"][\"train_mean_loss_list\"])):\n",
    "        for iidx in range(len(data[\"results\"][\"stage_1\"][\"train_mean_loss_list\"][idx])):\n",
    "            data[\"results\"][\"stage_1\"][\"train_mean_loss_list\"][idx][iidx] *= math.ceil(train_dataset_len / actual_batch_size_1)\n",
    "            data[\"results\"][\"stage_1\"][\"train_mean_loss_list\"][idx][iidx] /= train_dataset_len\n",
    "\n",
    "    for idx in range(len(data[\"results\"][\"stage_1\"][\"val_mean_loss_list\"])):\n",
    "        for iidx in range(len(data[\"results\"][\"stage_1\"][\"val_mean_loss_list\"][idx])):\n",
    "            data[\"results\"][\"stage_1\"][\"val_mean_loss_list\"][idx][iidx] *= math.ceil(val_dataset_len / actual_batch_size_1)\n",
    "            data[\"results\"][\"stage_1\"][\"val_mean_loss_list\"][idx][iidx] /= val_dataset_len\n",
    "\n",
    "    for idx in range(len(data[\"results\"][\"stage_2\"][\"train_mean_loss_list\"])):\n",
    "        for iidx in range(len(data[\"results\"][\"stage_2\"][\"train_mean_loss_list\"][idx])):\n",
    "            data[\"results\"][\"stage_2\"][\"train_mean_loss_list\"][idx][iidx] *= math.ceil(train_dataset_len / actual_batch_size_2)\n",
    "            data[\"results\"][\"stage_2\"][\"train_mean_loss_list\"][idx][iidx] /= train_dataset_len\n",
    "\n",
    "    for idx in range(len(data[\"results\"][\"stage_2\"][\"val_mean_loss_list\"])):\n",
    "        for iidx in range(len(data[\"results\"][\"stage_2\"][\"val_mean_loss_list\"][idx])):\n",
    "            data[\"results\"][\"stage_2\"][\"val_mean_loss_list\"][idx][iidx] *= math.ceil(val_dataset_len / actual_batch_size_2)\n",
    "            data[\"results\"][\"stage_2\"][\"val_mean_loss_list\"][idx][iidx] /= val_dataset_len\n",
    "\n",
    "    save_json_dict(data_filename, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNeXt Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_006/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_011/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_010/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_003/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_002/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_005/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_012/exp_data_corr.json\n",
      "batch_size_1 16\n",
      "grad_acc_iters_1 1\n",
      "num_gpus 8\n",
      "actual_batch_size_1 128\n",
      "corr_lr_1 0.0076\n",
      "base_lr_1 5.9375e-05\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_007/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_004/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_000/exp_data_corr.json\n",
      "batch_size_1 16\n",
      "grad_acc_iters_1 1\n",
      "num_gpus 4\n",
      "actual_batch_size_1 64\n",
      "corr_lr_1 0.0038\n",
      "base_lr_1 5.9375e-05\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_001/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_008/exp_data_corr.json\n",
      "/home-net/gortega/data/fashion_retrieval/ret_train/convnext_t/test_009/exp_data_corr.json\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(pathlib.Path.home(), \"data\", \"fashion_retrieval\", \"ret_train\", \"convnext_t\")\n",
    "\n",
    "for data_subdir in os.listdir(data_dir):\n",
    "\n",
    "    data_filename = os.path.join(data_dir, data_subdir, \"exp_data_corr.json\")\n",
    "    print(data_filename)\n",
    "\n",
    "    data = load_json_dict(data_filename)\n",
    "\n",
    "    #\n",
    "\n",
    "    num_gpus = len(data[\"settings\"][\"gpu_usage\"])\n",
    "\n",
    "    batch_size_1 = data[\"settings\"][\"stage_1\"][\"data_loading\"][\"batch_size\"]\n",
    "    grad_acc_iters_1 = data[\"settings\"][\"stage_1\"].get(\"max_acc_iter\", 1)\n",
    "    corr_lr_1 = data[\"settings\"][\"stage_1\"][\"learning_rate_list\"][0]\n",
    "\n",
    "    batch_size_2 = data[\"settings\"][\"stage_2\"][\"data_loading\"][\"batch_size\"]\n",
    "    grad_acc_iters_2 = data[\"settings\"][\"stage_2\"].get(\"max_acc_iter\", 1)\n",
    "    corr_lr_2 = data[\"settings\"][\"stage_2\"][\"learning_rate_list\"][0]\n",
    "\n",
    "    actual_batch_size_1 = batch_size_1 * grad_acc_iters_1 * num_gpus\n",
    "    actual_batch_size_2 = batch_size_2 * grad_acc_iters_2 * num_gpus\n",
    "\n",
    "    base_lr_1 = corr_lr_1 / actual_batch_size_1\n",
    "    base_lr_2 = corr_lr_2 / actual_batch_size_2\n",
    "\n",
    "    if \"012\" in data_filename or \"000\" in data_filename:\n",
    "        print(\"batch_size_1\", batch_size_1)\n",
    "        print(\"grad_acc_iters_1\", grad_acc_iters_1)\n",
    "        print(\"num_gpus\", num_gpus)\n",
    "        print(\"actual_batch_size_1\", actual_batch_size_1)\n",
    "        print(\"corr_lr_1\", corr_lr_1)\n",
    "        print(\"base_lr_1\", base_lr_1)\n",
    "\n",
    "    #\n",
    "\n",
    "    if \"max_acc_iter\" in data[\"settings\"][\"stage_1\"].keys():\n",
    "        del data[\"settings\"][\"stage_1\"][\"max_acc_iter\"]\n",
    "        data[\"settings\"][\"stage_1\"][\"data_loading\"][\"grad_acc_iters\"] = grad_acc_iters_1\n",
    "\n",
    "    if \"max_acc_iter\" in data[\"settings\"][\"stage_2\"].keys():\n",
    "        del data[\"settings\"][\"stage_2\"][\"max_acc_iter\"]\n",
    "        data[\"settings\"][\"stage_2\"][\"data_loading\"][\"grad_acc_iters\"] = grad_acc_iters_2\n",
    "\n",
    "    data[\"settings\"][\"stage_1\"][\"base_lr\"] = base_lr_1\n",
    "    data[\"settings\"][\"stage_2\"][\"base_lr\"] = base_lr_2\n",
    "\n",
    "    for idx in range(len(data[\"results\"][\"stage_1\"][\"train_mean_loss_list\"])):\n",
    "        for iidx in range(len(data[\"results\"][\"stage_1\"][\"train_mean_loss_list\"][idx])):\n",
    "            data[\"results\"][\"stage_1\"][\"train_mean_loss_list\"][idx][iidx] *= math.ceil(train_dataset_len / actual_batch_size_1)\n",
    "            data[\"results\"][\"stage_1\"][\"train_mean_loss_list\"][idx][iidx] /= train_dataset_len\n",
    "\n",
    "    for idx in range(len(data[\"results\"][\"stage_1\"][\"val_mean_loss_list\"])):\n",
    "        for iidx in range(len(data[\"results\"][\"stage_1\"][\"val_mean_loss_list\"][idx])):\n",
    "            data[\"results\"][\"stage_1\"][\"val_mean_loss_list\"][idx][iidx] *= math.ceil(val_dataset_len / actual_batch_size_1)\n",
    "            data[\"results\"][\"stage_1\"][\"val_mean_loss_list\"][idx][iidx] /= val_dataset_len\n",
    "\n",
    "    for idx in range(len(data[\"results\"][\"stage_2\"][\"train_mean_loss_list\"])):\n",
    "        for iidx in range(len(data[\"results\"][\"stage_2\"][\"train_mean_loss_list\"][idx])):\n",
    "            data[\"results\"][\"stage_2\"][\"train_mean_loss_list\"][idx][iidx] *= math.ceil(train_dataset_len / actual_batch_size_2)\n",
    "            data[\"results\"][\"stage_2\"][\"train_mean_loss_list\"][idx][iidx] /= train_dataset_len\n",
    "\n",
    "    for idx in range(len(data[\"results\"][\"stage_2\"][\"val_mean_loss_list\"])):\n",
    "        for iidx in range(len(data[\"results\"][\"stage_2\"][\"val_mean_loss_list\"][idx])):\n",
    "            data[\"results\"][\"stage_2\"][\"val_mean_loss_list\"][idx][iidx] *= math.ceil(val_dataset_len / actual_batch_size_2)\n",
    "            data[\"results\"][\"stage_2\"][\"val_mean_loss_list\"][idx][iidx] /= val_dataset_len\n",
    "\n",
    "    save_json_dict(data_filename, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1d9cd8eb66510c5ec86eb907d6561b8001175da1689fbe0f45c40d854d32b14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
