{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import global_local_attention_module_pytorch\n",
    "\n",
    "import src.utils.module\n",
    "import src.utils.comps\n",
    "\n",
    "import src.comps.heads_pyramid_2\n",
    "import src.comps.heads_glam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion L1234 1024 head                             1474560\n",
      "GLAM L4 1024 head                                  2521932\n",
      "GLAM Fusion L1234 1024 head                        2386648\n",
      "GLAM Fusion L1234 1440 head                        2544672\n",
      "GLAM Fusion L1234 1440 2G FC head                  2228616\n",
      "GLAM Fusion L1234 1440 No FC head                  2306568\n"
     ]
    }
   ],
   "source": [
    "feat_shapes = [\n",
    "    (96, 56, 56),\n",
    "    (192, 28, 28),\n",
    "    (384, 14, 14),\n",
    "    (768, 7, 7)\n",
    "]\n",
    "\n",
    "# Fusion L1234 1024 head\n",
    "\n",
    "head = src.comps.heads_pyramid_2.RetrievalHeadPyramidTopDownInstantSimple(\n",
    "    feat_shapes,\n",
    "    in_feat_idxs=[0, 1, 2, 3],\n",
    "    emb_size=1024\n",
    ")\n",
    "\n",
    "print(\"{:50s} {:d}\".format(\n",
    "    \"Fusion L1234 1024 head\",\n",
    "    src.utils.module.get_num_params(head)\n",
    "))\n",
    "\n",
    "# GLAM L4 1024 head\n",
    "\n",
    "head = src.comps.heads_glam.RetrievalHeadGLAM(\n",
    "    feat_shapes[-1],\n",
    "    emb_size=1024,\n",
    "    glam_int_channels=192\n",
    ")\n",
    "\n",
    "print(\"{:50s} {:d}\".format(\n",
    "    \"GLAM L4 1024 head\",\n",
    "    src.utils.module.get_num_params(head)\n",
    "))\n",
    "\n",
    "# GLAM Fusion L1234 1024 head\n",
    "\n",
    "head = src.comps.heads_glam.RetrievalGLAMHeadPyramidTopDownInstantSimple(\n",
    "    feat_shapes,\n",
    "    in_feat_idxs=[0, 1, 2, 3],\n",
    "    emb_size=1024,\n",
    "    glam_int_channels_list=[24, 48, 48, 96]\n",
    ")\n",
    "\n",
    "input_tensors = [torch.rand(size=[1] + list(feat_shape)) for feat_shape in feat_shapes]\n",
    "output_tensor = head(input_tensors)\n",
    "\n",
    "print(\"{:50s} {:d}\".format(\n",
    "    \"GLAM Fusion L1234 1024 head\",\n",
    "    src.utils.module.get_num_params(head)\n",
    "))\n",
    "\n",
    "# GLAM Fusion L1234 1440 head\n",
    "\n",
    "head = src.comps.heads_glam.RetrievalGLAMHeadPyramidTopDownInstantSimple(\n",
    "    feat_shapes,\n",
    "    in_feat_idxs=[0, 1, 2, 3],\n",
    "    emb_size=1440,\n",
    "    glam_int_channels_list=[24, 24, 48, 48]\n",
    ")\n",
    "\n",
    "input_tensors = [torch.rand(size=[1] + list(feat_shape)) for feat_shape in feat_shapes]\n",
    "output_tensor = head(input_tensors)\n",
    "\n",
    "print(\"{:50s} {:d}\".format(\n",
    "    \"GLAM Fusion L1234 1440 head\",\n",
    "    src.utils.module.get_num_params(head)\n",
    "))\n",
    "\n",
    "# GLAM Fusion L1234 1440 2G FC head\n",
    "\n",
    "head = src.comps.heads_glam.RetrievalGLAMHeadPyramidTopDownInstantSimple(\n",
    "    feat_shapes,\n",
    "    in_feat_idxs=[0, 1, 2, 3],\n",
    "    emb_size=1440,\n",
    "    glam_int_channels_list=[24, 48, 96, 96],\n",
    "    conv1_groups=2\n",
    ")\n",
    "\n",
    "input_tensors = [torch.rand(size=[1] + list(feat_shape)) for feat_shape in feat_shapes]\n",
    "output_tensor = head(input_tensors)\n",
    "\n",
    "print(\"{:50s} {:d}\".format(\n",
    "    \"GLAM Fusion L1234 1440 2G FC head\",\n",
    "    src.utils.module.get_num_params(head)\n",
    "))\n",
    "\n",
    "# GLAM Fusion L1234 1440 No FC head\n",
    "\n",
    "head = src.comps.heads_glam.RetrievalGLAMHeadPyramidTopDownInstantSimple(\n",
    "    feat_shapes,\n",
    "    in_feat_idxs=[0, 1, 2, 3],\n",
    "    emb_size=1440,\n",
    "    glam_int_channels_list=[24, 48, 96, 192],\n",
    "    conv1_groups=None\n",
    ")\n",
    "\n",
    "input_tensors = [torch.rand(size=[1] + list(feat_shape)) for feat_shape in feat_shapes]\n",
    "output_tensor = head(input_tensors)\n",
    "\n",
    "print(\"{:50s} {:d}\".format(\n",
    "    \"GLAM Fusion L1234 1440 No FC head\",\n",
    "    src.utils.module.get_num_params(head)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "649142fb4cdab8a2d2387ea4a1c8e262f08b2b20e4af0e114d36ea602bf8b868"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
