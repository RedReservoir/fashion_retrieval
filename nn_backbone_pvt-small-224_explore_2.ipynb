{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision import models\n",
    "import transformers\n",
    "from transformers import PvtModel, AutoImageProcessor\n",
    "\n",
    "from fir.datasets import deep_fashion_ctsrbm\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PvtModel.from_pretrained(\"Zetatech/pvt-small-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctsrbm_image_transform = AutoImageProcessor.from_pretrained(\"Zetatech/pvt-small-224\")\n",
    "ctsrbm_image_transform.size[\"height\"] = 448\n",
    "ctsrbm_image_transform.size[\"width\"] = 448\n",
    "ctsrbm_image_transform_corr = lambda t: torch.from_numpy(ctsrbm_image_transform(t).pixel_values[0])\n",
    "ctsrbm_dataset_dir = os.path.join(pathlib.Path.home(), \"data\", \"DeepFashion\", \"Consumer-to-shop Clothes Retrieval Benchmark\")\n",
    "ctsrbm_dataset = deep_fashion_ctsrbm.ConsToShopClothRetrBmkImageLoader(ctsrbm_dataset_dir, ctsrbm_image_transform_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PvtImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"PvtImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 448,\n",
       "    \"width\": 448\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctsrbm_image_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = ctsrbm_dataset[0]\n",
    "\n",
    "img_tensor = input[0]\n",
    "item_id = input[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24444"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fd0540776d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output\n",
    "    return hook\n",
    "\n",
    "model.encoder.block[0].register_forward_hook(get_activation('encoder.block[0]'))\n",
    "model.encoder.block[1].register_forward_hook(get_activation('encoder.block[1]'))\n",
    "model.encoder.block[2].register_forward_hook(get_activation('encoder.block[2]'))\n",
    "model.encoder.block[3].register_forward_hook(get_activation('encoder.block[3]'))\n",
    "model.encoder.layer_norm.register_forward_hook(get_activation('encoder.layer_norm'))\n",
    "model.encoder.register_forward_hook(get_activation('encoder'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOLA\n",
      "\n",
      "batch_size: 1\n",
      "num_channels: 3\n",
      "height: 224\n",
      "width: 224\n",
      "\n",
      "height: 56\n",
      "width: 56\n",
      "\n",
      "embeddings.shape: torch.Size([1, 3136, 64])\n",
      "\n",
      "self.cls_token is None\n",
      "\n",
      "  CALL: interpolate_pos_encoding\n",
      "  embeddings.shape: torch.Size([1, 3136, 64])\n",
      "  height: 56\n",
      "  width: 56\n",
      "\n",
      "  num_patches: 3136\n",
      "  self.config.image_size: 224\n",
      "  self.config.image_size ** 2: 50176\n",
      "\n",
      "  embeddings.shape: torch.Size([1, 3136, 64])\n",
      "  embeddings.shape: torch.Size([1, 64, 56, 56])\n",
      "  interpolated_embeddings.shape: torch.Size([1, 64, 56, 56])\n",
      "  interpolated_embeddings.shape: torch.Size([1, 3136, 64])\n",
      "\n",
      "self.position_embeddings.shape: torch.Size([1, 3136, 64])\n",
      "position_embeddings.shape: torch.Size([1, 3136, 64])\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOLA\n",
      "\n",
      "batch_size: 1\n",
      "num_channels: 64\n",
      "height: 56\n",
      "width: 56\n",
      "\n",
      "height: 28\n",
      "width: 28\n",
      "\n",
      "embeddings.shape: torch.Size([1, 784, 128])\n",
      "\n",
      "self.cls_token is None\n",
      "\n",
      "  CALL: interpolate_pos_encoding\n",
      "  embeddings.shape: torch.Size([1, 784, 128])\n",
      "  height: 28\n",
      "  width: 28\n",
      "\n",
      "  num_patches: 784\n",
      "  self.config.image_size: 224\n",
      "  self.config.image_size ** 2: 50176\n",
      "\n",
      "  embeddings.shape: torch.Size([1, 784, 128])\n",
      "  embeddings.shape: torch.Size([1, 128, 28, 28])\n",
      "  interpolated_embeddings.shape: torch.Size([1, 128, 28, 28])\n",
      "  interpolated_embeddings.shape: torch.Size([1, 784, 128])\n",
      "\n",
      "self.position_embeddings.shape: torch.Size([1, 784, 128])\n",
      "position_embeddings.shape: torch.Size([1, 784, 128])\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOLA\n",
      "\n",
      "batch_size: 1\n",
      "num_channels: 128\n",
      "height: 28\n",
      "width: 28\n",
      "\n",
      "height: 14\n",
      "width: 14\n",
      "\n",
      "embeddings.shape: torch.Size([1, 196, 320])\n",
      "\n",
      "self.cls_token is None\n",
      "\n",
      "  CALL: interpolate_pos_encoding\n",
      "  embeddings.shape: torch.Size([1, 196, 320])\n",
      "  height: 14\n",
      "  width: 14\n",
      "\n",
      "  num_patches: 196\n",
      "  self.config.image_size: 224\n",
      "  self.config.image_size ** 2: 50176\n",
      "\n",
      "  embeddings.shape: torch.Size([1, 196, 320])\n",
      "  embeddings.shape: torch.Size([1, 320, 14, 14])\n",
      "  interpolated_embeddings.shape: torch.Size([1, 320, 14, 14])\n",
      "  interpolated_embeddings.shape: torch.Size([1, 196, 320])\n",
      "\n",
      "self.position_embeddings.shape: torch.Size([1, 196, 320])\n",
      "position_embeddings.shape: torch.Size([1, 196, 320])\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "HOLA\n",
      "\n",
      "batch_size: 1\n",
      "num_channels: 320\n",
      "height: 14\n",
      "width: 14\n",
      "\n",
      "height: 7\n",
      "width: 7\n",
      "\n",
      "embeddings.shape: torch.Size([1, 49, 512])\n",
      "\n",
      "self.cls_token is not None\n",
      "\n",
      "  CALL: interpolate_pos_encoding\n",
      "  embeddings.shape: torch.Size([1, 49, 512])\n",
      "  height: 7\n",
      "  width: 7\n",
      "\n",
      "  num_patches: 49\n",
      "  self.config.image_size: 224\n",
      "  self.config.image_size ** 2: 50176\n",
      "\n",
      "  embeddings.shape: torch.Size([1, 49, 512])\n",
      "  embeddings.shape: torch.Size([1, 512, 7, 7])\n",
      "  interpolated_embeddings.shape: torch.Size([1, 512, 7, 7])\n",
      "  interpolated_embeddings.shape: torch.Size([1, 49, 512])\n",
      "\n",
      "self.position_embeddings.shape: torch.Size([1, 50, 512])\n",
      "position_embeddings.shape: torch.Size([1, 50, 512])\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = ctsrbm_dataset[0][0][None, :, :224, :224]\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOLA\n",
      "\n",
      "batch_size: 1\n",
      "num_channels: 3\n",
      "height: 448\n",
      "width: 448\n",
      "\n",
      "height: 112\n",
      "width: 112\n",
      "\n",
      "embeddings.shape: torch.Size([1, 12544, 64])\n",
      "\n",
      "self.cls_token is None\n",
      "\n",
      "  CALL: interpolate_pos_encoding\n",
      "  embeddings.shape: torch.Size([1, 3136, 64])\n",
      "  height: 112\n",
      "  width: 112\n",
      "\n",
      "  num_patches: 12544\n",
      "  self.config.image_size: 224\n",
      "  self.config.image_size ** 2: 50176\n",
      "\n",
      "  embeddings.shape: torch.Size([1, 3136, 64])\n",
      "  embeddings.shape: torch.Size([1, 16, 112, 112])\n",
      "  interpolated_embeddings.shape: torch.Size([1, 16, 112, 112])\n",
      "  interpolated_embeddings.shape: torch.Size([1, 12544, 16])\n",
      "\n",
      "self.position_embeddings.shape: torch.Size([1, 3136, 64])\n",
      "position_embeddings.shape: torch.Size([1, 12544, 16])\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m ctsrbm_dataset[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39mNone\u001b[39;00m, :, :\u001b[39m448\u001b[39m, :\u001b[39m448\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m output \u001b[39m=\u001b[39m model(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/fashion_retrieval/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/fashion_retrieval/.venv/lib/python3.8/site-packages/transformers/models/pvt/modeling_pvt.py:608\u001b[0m, in \u001b[0;36mPvtModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    603\u001b[0m output_hidden_states \u001b[39m=\u001b[39m (\n\u001b[1;32m    604\u001b[0m     output_hidden_states \u001b[39mif\u001b[39;00m output_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39moutput_hidden_states\n\u001b[1;32m    605\u001b[0m )\n\u001b[1;32m    606\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 608\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    609\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m    610\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    611\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    612\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    613\u001b[0m )\n\u001b[1;32m    614\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/fashion_retrieval/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1538\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1536\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1538\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1539\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1540\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1541\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1542\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1543\u001b[0m     ):\n",
      "File \u001b[0;32m~/fashion_retrieval/.venv/lib/python3.8/site-packages/transformers/models/pvt/modeling_pvt.py:475\u001b[0m, in \u001b[0;36mPvtEncoder.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    472\u001b[0m hidden_states \u001b[39m=\u001b[39m pixel_values\n\u001b[1;32m    473\u001b[0m \u001b[39mfor\u001b[39;00m idx, (embedding_layer, block_layer) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_embeddings, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock)):\n\u001b[1;32m    474\u001b[0m     \u001b[39m# first, obtain patch embeddings\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     hidden_states, height, width \u001b[39m=\u001b[39m embedding_layer(hidden_states)\n\u001b[1;32m    476\u001b[0m     \u001b[39m# second, send embeddings through blocks\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m block_layer:\n",
      "File \u001b[0;32m~/fashion_retrieval/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/fashion_retrieval/.venv/lib/python3.8/site-packages/transformers/models/pvt/modeling_pvt.py:191\u001b[0m, in \u001b[0;36mPvtPatchEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mposition_embeddings.shape:\u001b[39m\u001b[39m\"\u001b[39m, position_embeddings\u001b[39m.\u001b[39mshape) \u001b[39m# DEBUG\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[39mprint\u001b[39m() \u001b[39m# DEBUG\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(embeddings \u001b[39m+\u001b[39;49m position_embeddings)\n\u001b[1;32m    193\u001b[0m \u001b[39mprint\u001b[39m() \u001b[39m# DEBUG\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m---\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m# DEBUG\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (16) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "input = ctsrbm_dataset[0][0][None, :, :448, :448]\n",
    "output = model(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "649142fb4cdab8a2d2387ea4a1c8e262f08b2b20e4af0e114d36ea602bf8b868"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
