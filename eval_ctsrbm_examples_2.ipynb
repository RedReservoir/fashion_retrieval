{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import pathlib\n",
    "import pickle as pkl\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from datasets import deep_fashion_ctsrbm\n",
    "from arch import backbones, models, heads\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import utils.mem\n",
    "import utils.list\n",
    "import utils.train\n",
    "import utils.time\n",
    "import utils.log\n",
    "import utils.dict\n",
    "import utils.sig\n",
    "import utils.pkl\n",
    "import utils.chunk\n",
    "import utils.arr\n",
    "import utils.ten\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "from itertools import chain\n",
    "from functools import reduce\n",
    "\n",
    "import json\n",
    "import socket\n",
    "\n",
    "import pprint\n",
    "\n",
    "\n",
    "\n",
    "def print_tensor_info(tensor, name, logger):\n",
    "\n",
    "    logger.print(\n",
    "        \"{:s}:\".format(name),\n",
    "        tensor.shape,\n",
    "        tensor.dtype,\n",
    "        tensor.device,\n",
    "        utils.mem.sprint_fancy_num_bytes(utils.mem.get_num_bytes(tensor))\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# COMPONENT FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def create_backbone(backbone_class):\n",
    "\n",
    "    if backbone_class == \"ResNet50Backbone\":\n",
    "        backbone = backbones.ResNet50Backbone()\n",
    "    if backbone_class == \"EfficientNetB3Backbone\":\n",
    "        backbone = backbones.EfficientNetB3Backbone()\n",
    "    if backbone_class == \"EfficientNetB4Backbone\":\n",
    "        backbone = backbones.EfficientNetB4Backbone()\n",
    "    if backbone_class == \"EfficientNetB5Backbone\":\n",
    "        backbone = backbones.EfficientNetB5Backbone()\n",
    "    if backbone_class == \"ConvNeXtTinyBackbone\":\n",
    "        backbone = backbones.ConvNeXtTinyBackbone(contiguous_after_permute=True)\n",
    "\n",
    "    return backbone\n",
    "\n",
    "\n",
    "def load_experiment_checkpoint(\n",
    "        experiment_checkpoint_filename,\n",
    "        exp_params,\n",
    "        device\n",
    "        ):\n",
    "\n",
    "    # Load checkpoint\n",
    "\n",
    "    experiment_checkpoint = torch.load(experiment_checkpoint_filename)\n",
    "\n",
    "    # Backbone\n",
    "\n",
    "    backbone_class = exp_params[\"settings\"][\"backbone\"][\"class\"]\n",
    "\n",
    "    backbone = create_backbone(backbone_class).to(device)\n",
    "\n",
    "    backbone.load_state_dict(experiment_checkpoint[\"backbone_state_dict\"])\n",
    "\n",
    "    # Head\n",
    "\n",
    "    ret_head = heads.RetHead(backbone.out_shape, 1024).to(device)\n",
    "\n",
    "    ret_head.load_state_dict(experiment_checkpoint[\"ret_head_state_dict\"])\n",
    "\n",
    "    return (backbone, ret_head)\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# DATASET FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def create_backbone_transform(backbone_class):\n",
    "\n",
    "    if backbone_class == \"ResNet50Backbone\":\n",
    "        backbone_image_transform = torchvision.models.ResNet50_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"EfficientNetB3Backbone\":\n",
    "        backbone_image_transform = torchvision.models.EfficientNet_B3_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"EfficientNetB4Backbone\":\n",
    "        backbone_image_transform = torchvision.models.EfficientNet_B4_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"EfficientNetB5Backbone\":\n",
    "        backbone_image_transform = torchvision.models.EfficientNet_B5_Weights.DEFAULT.transforms()\n",
    "    if backbone_class == \"ConvNeXtTinyBackbone\":\n",
    "        backbone_image_transform = torchvision.models.ConvNeXt_Tiny_Weights.DEFAULT.transforms()\n",
    "\n",
    "    return backbone_image_transform\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# JSON DATA FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def save_json_data(\n",
    "        json_data_filename,\n",
    "        json_data\n",
    "        ):\n",
    "\n",
    "    with open(json_data_filename, 'w') as json_data_file:\n",
    "        json.dump(json_data, json_data_file, indent=2)\n",
    "\n",
    "\n",
    "def load_json_data(\n",
    "        json_data_filename\n",
    "        ):\n",
    "\n",
    "    with open(json_data_filename, 'r') as json_data_file:\n",
    "        json_data = json.load(json_data_file)\n",
    "\n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "# PERFORMANCE FUNCTIONS\n",
    "########\n",
    "\n",
    "\n",
    "\n",
    "def compute_embeddings_and_item_ids(\n",
    "    ret_model,\n",
    "    image_loader,\n",
    "    device,\n",
    "    with_tqdm\n",
    "):\n",
    "\n",
    "    ret_model.eval()\n",
    "\n",
    "    # Embedding calculation\n",
    "\n",
    "    all_img_embs = torch.tensor([], dtype=float).to(device)\n",
    "    all_item_ids = torch.tensor([], dtype=int).to(device)\n",
    "\n",
    "    loader_gen = image_loader\n",
    "    if with_tqdm: loader_gen = tqdm(loader_gen)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in loader_gen:\n",
    "\n",
    "            # img_embs\n",
    "\n",
    "            imgs = batch[0].to(device)\n",
    "            img_embs = ret_model(imgs)\n",
    "            all_img_embs = torch.cat([all_img_embs, img_embs])\n",
    "\n",
    "            # item_ids\n",
    "\n",
    "            item_ids = batch[1].to(device)\n",
    "            all_item_ids = torch.cat([all_item_ids, item_ids])\n",
    "\n",
    "    return all_img_embs, all_item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_performance_metrics(\n",
    "    shop_img_embs,\n",
    "    shop_item_ids,\n",
    "    cons_img_embs,\n",
    "    cons_item_ids,\n",
    "    k_values,\n",
    "    cons_imgs_chunk_size,\n",
    "    with_tqdm\n",
    "):\n",
    "\n",
    "    num_cons_imgs = cons_img_embs.shape[0]\n",
    "\n",
    "    avg_p_at_k_dict = {k: 0 for k in k_values}\n",
    "    avg_r_at_k_dict = {k: 0 for k in k_values}\n",
    "\n",
    "    # Precision and recall metrics\n",
    "\n",
    "    ## [i]: Number of shop items with the same item id as cons img i\n",
    "\n",
    "    cons_shop_item_id_counts = torch.empty_like(cons_item_ids)\n",
    "\n",
    "    for idx, cons_item_id in enumerate(cons_item_ids):\n",
    "        counts = torch.sum(torch.eq(shop_item_ids, cons_item_id))\n",
    "        cons_shop_item_id_counts[idx] = counts\n",
    "\n",
    "    cons_img_idxs_chunk_gen = utils.chunk.chunk_partition_size(np.arange(num_cons_imgs), cons_imgs_chunk_size)\n",
    "    if with_tqdm: cons_img_idxs_chunk_gen = tqdm(cons_img_idxs_chunk_gen)\n",
    "\n",
    "    for cons_img_idxs_chunk in cons_img_idxs_chunk_gen:\n",
    "\n",
    "        ## [i, j]: Distance from shop img i to cons img j\n",
    "\n",
    "        shop_to_cons_dists = torch.cdist(shop_img_embs, cons_img_embs[cons_img_idxs_chunk, :])\n",
    "\n",
    "        ## [:, i]: Ordered closest shop img idxs to cons img i\n",
    "\n",
    "        shop_to_cons_ordered_idxs = torch.argsort(shop_to_cons_dists, dim=0)\n",
    "\n",
    "        ## [:, i]: ordered closest shop img item ids to cons img i \n",
    "\n",
    "        shop_to_cons_nearest_item_ids = shop_item_ids[shop_to_cons_ordered_idxs]\n",
    "\n",
    "        ## [:, i]: True/False if, for each shop image, the cons img i is of the same item id\n",
    "\n",
    "        shop_to_cons_hits = torch.eq(shop_to_cons_nearest_item_ids, cons_item_ids[cons_img_idxs_chunk])\n",
    "\n",
    "        for k in k_values:\n",
    "\n",
    "            k_corr = shop_img_embs.shape[0] if k == -1 else k\n",
    "\n",
    "            ## [i]: Number of hits of cons img i (out of the k first)\n",
    "\n",
    "            shop_to_cons_hits_sum = torch.sum(shop_to_cons_hits[:k_corr, ], dim=0)\n",
    "\n",
    "            ## [i]: p/r_at_k of cons img i\n",
    "\n",
    "            p_at_k = shop_to_cons_hits_sum / k_corr\n",
    "            r_at_k = shop_to_cons_hits_sum / cons_shop_item_id_counts[cons_img_idxs_chunk]\n",
    "\n",
    "            ## Accumulate results\n",
    "\n",
    "            avg_p_at_k_dict[k] += torch.sum(p_at_k).item()\n",
    "            avg_r_at_k_dict[k] += torch.sum(r_at_k).item()\n",
    "\n",
    "    ## Average results\n",
    "    \n",
    "    for k in k_values:\n",
    "\n",
    "        avg_p_at_k_dict[k] /= num_cons_imgs\n",
    "        avg_r_at_k_dict[k] /= num_cons_imgs\n",
    "\n",
    "    # Composite metrics\n",
    "\n",
    "    avg_f1_at_k_dict = {k: 2 / ((1 / avg_p_at_k_dict[k]) + (1 / avg_r_at_k_dict[k])) for k in k_values}\n",
    "\n",
    "    return avg_p_at_k_dict, avg_r_at_k_dict, avg_f1_at_k_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_closest_idxs(\n",
    "    shop_img_embs,\n",
    "    shop_img_idxs,\n",
    "    shop_item_ids,\n",
    "    cons_img_embs,\n",
    "    cons_img_idxs,\n",
    "    cons_item_ids,\n",
    "    desired_cons_img_idxs,\n",
    "    num_desired_shop_imgs,\n",
    "    cons_imgs_chunk_size,\n",
    "    with_tqdm,\n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    shop_img_idxs = torch.tensor(shop_img_idxs).to(device)\n",
    "    shop_item_ids = shop_item_ids.to(device)\n",
    "    \n",
    "    # Result tensors\n",
    "\n",
    "    num_desired_cons_imgs = len(desired_cons_img_idxs)\n",
    "\n",
    "    ## [i, j]: j-th closest shop img idx to cons img with desired zidx i\n",
    "    ## [i, j]: j-th closest shop item id to cons img with desired zidx i\n",
    "    ## [i, j]: j-th closest shop dist to cons img with desired zidx i\n",
    "\n",
    "    shop_to_desired_cons_ordered_closest_img_idxs = torch.empty(size=(num_desired_cons_imgs, num_desired_shop_imgs), dtype=int).to(device)\n",
    "    shop_to_desired_cons_ordered_closest_item_ids = torch.empty(size=(num_desired_cons_imgs, num_desired_shop_imgs), dtype=int).to(device)\n",
    "    shop_to_desired_cons_ordered_closest_dists = torch.empty(size=(num_desired_cons_imgs, num_desired_shop_imgs), dtype=float).to(device)\n",
    "    \n",
    "    # Counts of correct shop images\n",
    "\n",
    "    ## [i]: Number of shop items with the same item id as cons img i\n",
    "\n",
    "    cons_shop_item_id_counts = torch.empty_like(cons_item_ids)\n",
    "\n",
    "    for idx, cons_item_id in enumerate(cons_item_ids):\n",
    "        counts = torch.sum(torch.eq(shop_item_ids, cons_item_id))\n",
    "        cons_shop_item_id_counts[idx] = counts\n",
    "\n",
    "    # Preparing desired_cons_img_zidxs chunks\n",
    "\n",
    "    desired_cons_img_zidxs = utils.arr.compute_zidxs(cons_img_idxs, desired_cons_img_idxs)\n",
    "    desired_cons_img_zzidxs = np.arange(num_desired_cons_imgs)\n",
    "    \n",
    "    desired_cons_img_zidxs_chunk_gen = utils.chunk.chunk_partition_size(desired_cons_img_zidxs, cons_imgs_chunk_size)\n",
    "    desired_cons_img_zzidxs_chunk_gen = utils.chunk.chunk_partition_size(desired_cons_img_zzidxs, cons_imgs_chunk_size)    \n",
    "    \n",
    "    chunk_idxs_gen = zip(desired_cons_img_zidxs_chunk_gen, desired_cons_img_zzidxs_chunk_gen)\n",
    "    if with_tqdm: chunk_idxs_gen = tqdm(chunk_idxs_gen)\n",
    "\n",
    "    for desired_cons_img_zidxs_chunk, desired_cons_img_zzidxs_chunk in chunk_idxs_gen:\n",
    "\n",
    "        ## [j, i]: Distance from shop img zidx j to cons img with zidx i\n",
    "\n",
    "        shop_to_cons_dists = torch.cdist(shop_img_embs, cons_img_embs[desired_cons_img_zidxs_chunk, :])\n",
    "\n",
    "        ## [:, i]: Ordered closest shop img zidxs to cons img with zidx i\n",
    "\n",
    "        shop_to_cons_ordered_closest_zidxs = torch.argsort(shop_to_cons_dists, dim=0).to(device)\n",
    "\n",
    "        ## [:, i]: Ordered closest shop img idxs to cons img with zidx i \n",
    "        ## [:, i]: Ordered closest shop item ids to cons img with zidx i \n",
    "        ## [:, i]: Ordered closest shop img distances to cons img with zidx i \n",
    "\n",
    "        shop_to_cons_ordered_closest_img_idxs = shop_img_idxs[shop_to_cons_ordered_closest_zidxs]\n",
    "        shop_to_cons_ordered_closest_item_ids = shop_item_ids[shop_to_cons_ordered_closest_zidxs]\n",
    "        shop_to_cons_ordered_closest_dists = torch.take_along_dim(shop_to_cons_dists, shop_to_cons_ordered_closest_zidxs, dim=0)\n",
    "\n",
    "        shop_to_desired_cons_ordered_closest_img_idxs[desired_cons_img_zzidxs_chunk, :] = torch.t(shop_to_cons_ordered_closest_img_idxs[:num_desired_shop_imgs, :])\n",
    "        shop_to_desired_cons_ordered_closest_item_ids[desired_cons_img_zzidxs_chunk, :] = torch.t(shop_to_cons_ordered_closest_item_ids[:num_desired_shop_imgs, :])\n",
    "        shop_to_desired_cons_ordered_closest_dists[desired_cons_img_zzidxs_chunk, :] = torch.t(shop_to_cons_ordered_closest_dists[:num_desired_shop_imgs, :])\n",
    "\n",
    "    # Indexing counts\n",
    "\n",
    "    cons_shop_item_id_counts = cons_shop_item_id_counts[desired_cons_img_zzidxs]\n",
    "\n",
    "    return (\n",
    "        shop_to_desired_cons_ordered_closest_img_idxs,\n",
    "        shop_to_desired_cons_ordered_closest_item_ids,\n",
    "        shop_to_desired_cons_ordered_closest_dists,\n",
    "        cons_shop_item_id_counts\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command arguments: /home-net/gortega/fashion_retrieval/.venv/lib/python3.8/site-packages/ipykernel_launcher.py --ip=127.0.0.1 --stdin=9014 --control=9012 --hb=9011 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"6ab54d6b-2a3e-4091-a67b-2f967de07c73\" --shell=9013 --transport=\"tcp\" --iopub=9015 --f=/home-net/gortega/.local/share/jupyter/runtime/kernel-v2-3309626jeQtbskLb910.json\n",
      "Selecting CUDA devices\n",
      "Selected CUDA devices\n",
      "Current memory usage:\n",
      "  Device ID  0:  95.875 MiB /  11.000 GiB (  0.85%) - NVIDIA GeForce GTX 1080 Ti\n",
      "Initializing image loader dataset\n",
      "Initialized image loader dataset\n",
      "Loading model from checkpoint\n",
      "Loaded model from checkpoint\n",
      "Loaded model from checkpoint\n",
      "Train split examples begin\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "datetime_now_str = datetime.now().strftime(\"%d-%m-%Y--%H:%M:%S\")\n",
    "\n",
    "\n",
    "####\n",
    "# COMMAND ARGUMENTS\n",
    "####\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"eval_params_filename\", help=\"filename of the evaluation params json file inside the \\\"eval_params\\\" directory\")\n",
    "parser.add_argument(\"exp_params_filename\", help=\"filename of the experiment params json file inside the \\\"exp_params\\\" directory\")\n",
    "parser.add_argument(\"--silent\", help=\"no terminal prints will be made\", action=\"store_true\")\n",
    "parser.add_argument(\"--notqdm\", help=\"no tqdm bars will be shown\", action=\"store_true\")\n",
    "\n",
    "command = \"python eval_ctsrbm_examples.py resnet_ret_train/test_50_000__eval_ctsrbm_examples.json resnet_ret_train/test_50_000__train_ret_DDP.json\"\n",
    "command_args = parser.parse_args(command.split()[2:])\n",
    "\n",
    "eval_params_filename = os.path.join(pathlib.Path.home(), \"fashion_retrieval\", \"exp_params\", command_args.eval_params_filename)\n",
    "exp_params_filename = os.path.join(pathlib.Path.home(), \"fashion_retrieval\", \"exp_params\", command_args.exp_params_filename)\n",
    "\n",
    "with_tqdm = not command_args.notqdm and not command_args.silent\n",
    "\n",
    "####\n",
    "# EVALUATION PREREQUISITES\n",
    "####\n",
    "\n",
    "\n",
    "# Read params\n",
    "\n",
    "eval_params = load_json_data(eval_params_filename)\n",
    "exp_params = load_json_data(exp_params_filename)\n",
    "\n",
    "# Experiment directory\n",
    "\n",
    "experiment_name__eval = eval_params[\"experiment_name\"]\n",
    "experiment_name__exp = exp_params[\"experiment_name\"]\n",
    "\n",
    "if experiment_name__eval != experiment_name__exp:\n",
    "    raise ValueError(\"Experiment names do not match\")\n",
    "\n",
    "\n",
    "####\n",
    "# PREPARE LOGGER\n",
    "####\n",
    "\n",
    "\n",
    "experiment_name = eval_params[\"experiment_name\"]\n",
    "experiment_dirname = os.path.join(pathlib.Path.home(), \"data\", \"fashion_retrieval\", experiment_name)\n",
    "\n",
    "log_filename = \"eval_ctsrbm_examples_logs.txt\"\n",
    "log_full_filename = os.path.join(experiment_dirname, log_filename)\n",
    "if os.path.exists(log_full_filename):\n",
    "    os.remove(log_full_filename)\n",
    "\n",
    "logger_streams = [log_full_filename]\n",
    "if not command_args.silent: logger_streams.append(sys.stdout)\n",
    "\n",
    "logger = utils.log.Logger(logger_streams)\n",
    "\n",
    "logger.print(\"Command arguments:\", \" \".join(sys.argv))\n",
    "\n",
    "\n",
    "####\n",
    "# PREPARE EVAL DATA\n",
    "####\n",
    "\n",
    "\n",
    "eval_data = {}\n",
    "\n",
    "eval_data[\"script_name\"] = \"eval_ctsrbm_examples.py\"\n",
    "eval_data[\"command_args\"] = sys.argv\n",
    "eval_data[\"experiment_name\"] = eval_params[\"experiment_name\"]\n",
    "eval_data[\"settings\"] = {}\n",
    "eval_data[\"results\"] = {}\n",
    "\n",
    "eval_data[\"settings\"][\"datasets\"] = [\n",
    "    \"DeepFashion Consumer-to-shop Clothes Retrieval Benchmark\"\n",
    "]\n",
    "\n",
    "eval_data[\"settings\"][\"model_checkpoint\"] = eval_params[\"settings\"][\"model_checkpoint\"]\n",
    "\n",
    "\n",
    "####\n",
    "# GPU INITIALIZATION\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Selecting CUDA devices\")\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "device_idx = eval_params[\"settings\"][\"device_idx\"]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(idx) for idx in [device_idx]])\n",
    "\n",
    "device = torch.device(0)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "eval_data[\"settings\"][\"gpu_usage\"] = utils.mem.list_gpu_data([device_idx])\n",
    "eval_data[\"settings\"][\"hostname\"] = socket.gethostname()\n",
    "\n",
    "logger.print(\"Selected CUDA devices\")\n",
    "\n",
    "logger.print(\"Current memory usage:\")\n",
    "logger.print(utils.mem.sprint_memory_usage([eval_params[\"settings\"][\"device_idx\"]], num_spaces=2))\n",
    "\n",
    "\n",
    "####\n",
    "# DATA INITIALIZATION\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Initializing image loader dataset\")\n",
    "\n",
    "# Dataset initialization\n",
    "\n",
    "backbone_class = exp_params[\"settings\"][\"backbone\"][\"class\"]\n",
    "\n",
    "backbone_image_transform = create_backbone_transform(backbone_class)\n",
    "backbone_image_transform.antialias = True\n",
    "\n",
    "ctsrbm_dataset_dir = os.path.join(pathlib.Path.home(), \"data\", \"DeepFashion\", \"Consumer-to-shop Clothes Retrieval Benchmark\")\n",
    "\n",
    "ctsrbm_dataset = deep_fashion_ctsrbm.ConsToShopClothRetrBmkImageLoader(ctsrbm_dataset_dir, img_transform=backbone_image_transform)\n",
    "\n",
    "logger.print(\"Initialized image loader dataset\")\n",
    "\n",
    "\n",
    "####\n",
    "# MODEL INITIALIZATION\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Loading model from checkpoint\")\n",
    "\n",
    "# Load components\n",
    "\n",
    "experiment_checkpoint_filename = eval_params[\"settings\"][\"model_checkpoint\"]\n",
    "\n",
    "experiment_checkpoint_filename_full = os.path.join(\n",
    "    experiment_dirname, experiment_checkpoint_filename\n",
    ")\n",
    "\n",
    "backbone, ret_head =\\\n",
    "load_experiment_checkpoint(\n",
    "    experiment_checkpoint_filename_full,\n",
    "    exp_params,\n",
    "    device\n",
    ")\n",
    "\n",
    "logger.print(\"Loaded model from checkpoint\")\n",
    "\n",
    "\n",
    "# Build models\n",
    "\n",
    "ret_model = models.BackboneAndHead(backbone, ret_head).to(device)\n",
    "\n",
    "logger.print(\"Loaded model from checkpoint\")\n",
    "\n",
    "\n",
    "####\n",
    "# TRAIN PERFORMANCE METRICS\n",
    "####\n",
    "\n",
    "\n",
    "logger.print(\"Train split examples begin\")\n",
    "\n",
    "# Data loader initialization\n",
    "\n",
    "train_shop_img_idxs = ctsrbm_dataset.get_subset_indices(split=\"train\", domain=\"shop\")\n",
    "train_cons_img_idxs = ctsrbm_dataset.get_subset_indices(split=\"train\", domain=\"consumer\")\n",
    "\n",
    "train_shop_dataset = Subset(ctsrbm_dataset, train_shop_img_idxs)\n",
    "train_cons_dataset = Subset(ctsrbm_dataset, train_cons_img_idxs)\n",
    "\n",
    "batch_size = eval_params[\"settings\"][\"data_loading\"][\"batch_size\"]\n",
    "num_workers = eval_params[\"settings\"][\"data_loading\"][\"num_workers\"]\n",
    "\n",
    "train_shop_loader = DataLoader(\n",
    "    train_shop_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_cons_loader = DataLoader(\n",
    "    train_cons_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Computing image embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 711/711 [00:40<00:00, 17.72it/s]\n",
      "100%|██████████| 3069/3069 [03:07<00:00, 16.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Computed image embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Embedding calculation\n",
    "\n",
    "logger.print(\"  Computing image embeddings\")\n",
    "\n",
    "train_shop_img_embs, train_shop_item_ids = compute_embeddings_and_item_ids(\n",
    "    ret_model,\n",
    "    train_shop_loader,\n",
    "    device,\n",
    "    with_tqdm\n",
    "    \n",
    ")\n",
    "\n",
    "train_cons_img_embs, train_cons_item_ids = compute_embeddings_and_item_ids(\n",
    "    ret_model,\n",
    "    train_cons_loader,\n",
    "    device,\n",
    "    with_tqdm\n",
    ")\n",
    "\n",
    "logger.print(\"  Computed image embeddings\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example calculation\n",
    "\n",
    "train_desired_cons_img_idxs = eval_params[\"settings\"][\"train\"][\"desired_cons_img_idxs\"]\n",
    "train_num_desired_shop_imgs = eval_params[\"settings\"][\"train\"][\"num_desired_shop_imgs\"]\n",
    "cons_imgs_chunk_size = utils.dict.chain_get(\n",
    "    eval_params,\n",
    "    \"settings\", \"cons_imgs_chunk_size\",\n",
    "    default=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[502, 1097, 1502, 2029, 2976]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desired_cons_img_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_num_desired_shop_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    shop_img_embs,\n",
    "    shop_img_idxs,\n",
    "    shop_item_ids,\n",
    "    cons_img_embs,\n",
    "    cons_img_idxs,\n",
    "    cons_item_ids,\n",
    "    desired_cons_img_idxs,\n",
    "    num_desired_shop_imgs,\n",
    "    cons_imgs_chunk_size,\n",
    "    with_tqdm,\n",
    "    device\n",
    ") = (\n",
    "    train_shop_img_embs,\n",
    "    train_shop_img_idxs,\n",
    "    train_shop_item_ids,\n",
    "    train_cons_img_embs,\n",
    "    train_cons_img_idxs,\n",
    "    train_cons_item_ids,\n",
    "    train_desired_cons_img_idxs,\n",
    "    train_num_desired_shop_imgs,\n",
    "    cons_imgs_chunk_size,\n",
    "    with_tqdm,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  12,  13,  14,  16,  30,  31,  36,  38,  40,  42,  64,  67,\n",
       "        75,  80,  87,  90,  91,  93,  96,  97, 105, 106, 109, 119, 126,\n",
       "       130, 137, 140, 141])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shop_img_idxs[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24444, 24434, 24434, 24434, 17990, 17669, 17669, 19192, 13695, 29336,\n",
       "        30193, 16092, 16092, 28284, 11377, 27615, 26989, 26989, 28410, 19550,\n",
       "        19550, 17648, 17648, 17238, 30464, 27322, 18358, 12862, 13744, 13744],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shop_item_ids[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 24444\n",
      "12 24434\n",
      "13 24434\n",
      "14 24434\n",
      "16 17990\n",
      "30 17669\n",
      "31 17669\n",
      "36 19192\n",
      "38 13695\n",
      "40 29336\n",
      "42 30193\n",
      "64 16092\n",
      "67 16092\n",
      "75 28284\n",
      "80 11377\n",
      "87 27615\n",
      "90 26989\n",
      "91 26989\n",
      "93 28410\n",
      "96 19550\n",
      "97 19550\n",
      "105 17648\n",
      "106 17648\n",
      "109 17238\n",
      "119 30464\n",
      "126 27322\n",
      "130 18358\n",
      "137 12862\n",
      "140 13744\n",
      "141 13744\n"
     ]
    }
   ],
   "source": [
    "for shop_img_idx, shop_item_id in zip(shop_img_idxs[:30], shop_item_ids[:30]):\n",
    "    print(shop_img_idx.item(), shop_item_id.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_img_idxs = torch.tensor(shop_img_idxs).to(device)\n",
    "shop_item_ids = shop_item_ids.to(device)\n",
    "\n",
    "# Result tensors\n",
    "\n",
    "num_desired_cons_imgs = len(desired_cons_img_idxs)\n",
    "\n",
    "## [i, j]: j-th closest shop img idx to cons img with desired zidx i\n",
    "## [i, j]: j-th closest shop item id to cons img with desired zidx i\n",
    "## [i, j]: j-th closest shop dist to cons img with desired zidx i\n",
    "\n",
    "shop_to_desired_cons_ordered_closest_img_idxs = torch.empty(size=(num_desired_cons_imgs, num_desired_shop_imgs), dtype=int).to(device)\n",
    "shop_to_desired_cons_ordered_closest_item_ids = torch.empty(size=(num_desired_cons_imgs, num_desired_shop_imgs), dtype=int).to(device)\n",
    "shop_to_desired_cons_ordered_closest_dists = torch.empty(size=(num_desired_cons_imgs, num_desired_shop_imgs), dtype=float).to(device)\n",
    "\n",
    "# Counts of correct shop images\n",
    "\n",
    "## [i]: Number of shop items with the same item id as cons img with zidx i\n",
    "\n",
    "cons_shop_item_id_counts = torch.empty_like(cons_item_ids)\n",
    "\n",
    "for cons_img_zidx, cons_item_id in enumerate(cons_item_ids):\n",
    "    counts = torch.sum(torch.eq(shop_item_ids, cons_item_id))\n",
    "    cons_shop_item_id_counts[cons_img_zidx] = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  8,  9, 10, 11, 15, 25, 26, 27, 28, 29, 32, 33, 34, 35,\n",
       "       37, 39, 41, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons_img_idxs[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24444, 24444, 24444, 24434, 24434, 24434, 24434, 17990, 17669, 17669,\n",
       "        17669, 17669, 17669, 17669, 19192, 19192, 19192, 13695, 29336, 30193,\n",
       "        16092, 16092, 16092, 16092, 16092, 16092, 16092, 16092, 16092, 16092],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons_item_ids[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 24444\n",
      "1 24444\n",
      "2 24444\n",
      "8 24434\n",
      "9 24434\n",
      "10 24434\n",
      "11 24434\n",
      "15 17990\n",
      "25 17669\n",
      "26 17669\n",
      "27 17669\n",
      "28 17669\n",
      "29 17669\n",
      "32 17669\n",
      "33 19192\n",
      "34 19192\n",
      "35 19192\n",
      "37 13695\n",
      "39 29336\n",
      "41 30193\n",
      "55 16092\n",
      "56 16092\n",
      "57 16092\n",
      "58 16092\n",
      "59 16092\n",
      "60 16092\n",
      "61 16092\n",
      "62 16092\n",
      "63 16092\n",
      "65 16092\n"
     ]
    }
   ],
   "source": [
    "for cons_img_idx, cons_item_id in zip(cons_img_idxs[:30], cons_item_ids[:30]):\n",
    "    print(cons_img_idx.item(), cons_item_id.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 3, 3, 3, 3, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2], device='cuda:0')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons_shop_item_id_counts[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing desired_cons_img_zidxs chunks\n",
    "\n",
    "desired_cons_img_zidxs = utils.arr.compute_zidxs(cons_img_idxs, desired_cons_img_idxs)\n",
    "desired_cons_img_zzidxs = np.arange(num_desired_cons_imgs)\n",
    "\n",
    "desired_cons_img_zidxs_chunk_gen = utils.chunk.chunk_partition_size(desired_cons_img_zidxs, cons_imgs_chunk_size)\n",
    "desired_cons_img_zzidxs_chunk_gen = utils.chunk.chunk_partition_size(desired_cons_img_zzidxs, cons_imgs_chunk_size)    \n",
    "\n",
    "chunk_idxs_gen = zip(desired_cons_img_zidxs_chunk_gen, desired_cons_img_zzidxs_chunk_gen)\n",
    "#if with_tqdm: chunk_idxs_gen = tqdm(chunk_idxs_gen)\n",
    "desired_cons_img_zidxs_chunk, desired_cons_img_zzidxs_chunk = next(chunk_idxs_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([186, 356, 498, 664, 971])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_cons_img_zidxs_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_cons_img_zzidxs_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[502, 1097, 1502, 2029, 2976]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_cons_img_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 502, 1097, 1502, 2029, 2976])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons_img_idxs[desired_cons_img_zidxs_chunk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22872, 16232, 14698, 27694, 27523], device='cuda:0')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons_item_ids[desired_cons_img_zidxs_chunk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [j, i]: Distance from shop img zidx j to cons img with zidx i\n",
    "\n",
    "shop_to_cons_dists = torch.cdist(shop_img_embs, cons_img_embs[desired_cons_img_zidxs_chunk, :])\n",
    "\n",
    "## [:, i]: Ordered closest shop img zidxs to cons img with zidx i\n",
    "\n",
    "shop_to_cons_ordered_closest_zidxs = torch.argsort(shop_to_cons_dists, dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: shop_to_cons_dists\n",
      "  shape:  torch.Size([22723, 5])\n",
      "  dtype:  torch.float64\n",
      "  device:  cuda:0\n",
      "  mem:  887.688 KiB\n"
     ]
    }
   ],
   "source": [
    "utils.ten.print_tensor_info(shop_to_cons_dists, \"shop_to_cons_dists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.3674, 6.8429, 5.9824,  ..., 8.7328, 6.3396, 6.3854], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shop_to_cons_dists[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.3674, 6.9056, 8.2916, 8.2196, 7.4310], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shop_to_cons_dists[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: shop_to_cons_ordered_closest_zidxs\n",
      "  shape:  torch.Size([22723, 5])\n",
      "  dtype:  torch.int64\n",
      "  device:  cuda:0\n",
      "  mem:  887.688 KiB\n"
     ]
    }
   ],
   "source": [
    "utils.ten.print_tensor_info(shop_to_cons_ordered_closest_zidxs, \"shop_to_cons_ordered_closest_zidxs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  264,   142, 10278,  ..., 17542, 17909,  2787], device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shop_to_cons_ordered_closest_zidxs[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  264, 10705,   178, 16297, 14571], device='cuda:0')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shop_to_cons_ordered_closest_zidxs[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ten = shop_to_cons_dists[:, 0][shop_to_cons_ordered_closest_zidxs[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: my_ten\n",
      "  shape:  torch.Size([22723])\n",
      "  dtype:  torch.float64\n",
      "  device:  cuda:0\n",
      "  mem:  177.594 KiB\n"
     ]
    }
   ],
   "source": [
    "utils.ten.print_tensor_info(my_ten, \"my_ten\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.6781, 4.7327, 4.8759, 4.8931, 4.9059, 4.9107, 4.9230, 4.9456, 4.9699,\n",
       "        4.9800], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ten[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [:, i]: Ordered closest shop img idxs to cons img with zidx i \n",
    "## [:, i]: Ordered closest shop item ids to cons img with zidx i \n",
    "## [:, i]: Ordered closest shop img distances to cons img with zidx i \n",
    "\n",
    "shop_to_cons_ordered_closest_img_idxs = shop_img_idxs[shop_to_cons_ordered_closest_zidxs]\n",
    "shop_to_cons_ordered_closest_item_ids = shop_item_ids[shop_to_cons_ordered_closest_zidxs]\n",
    "shop_to_cons_ordered_closest_dists = torch.take_along_dim(shop_to_cons_dists, shop_to_cons_ordered_closest_zidxs, dim=0)\n",
    "\n",
    "# Repeated comments\n",
    "## [i, j]: j-th closest shop img idx to cons img with desired zidx i\n",
    "## [i, j]: j-th closest shop item id to cons img with desired zidx i\n",
    "## [i, j]: j-th closest shop dist to cons img with desired zidx i\n",
    "\n",
    "shop_to_desired_cons_ordered_closest_img_idxs[desired_cons_img_zzidxs_chunk, :] = torch.t(shop_to_cons_ordered_closest_img_idxs[:num_desired_shop_imgs, :])\n",
    "shop_to_desired_cons_ordered_closest_item_ids[desired_cons_img_zzidxs_chunk, :] = torch.t(shop_to_cons_ordered_closest_item_ids[:num_desired_shop_imgs, :])\n",
    "shop_to_desired_cons_ordered_closest_dists[desired_cons_img_zzidxs_chunk, :] = torch.t(shop_to_cons_ordered_closest_dists[:num_desired_shop_imgs, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  2133,   1216, 106622, 186259, 124060, 156906,   1360,  98885,  93252,\n",
       "          3491], device='cuda:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shop_to_cons_ordered_closest_img_idxs[:num_desired_shop_imgs, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9151, 24290, 11877,  7704, 10576, 10718, 13258, 21744, 26467, 20749],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shop_to_cons_ordered_closest_item_ids[:num_desired_shop_imgs, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_shop_item_id_counts_orig = cons_shop_item_id_counts.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: cons_shop_item_id_counts_orig\n",
      "  shape:  torch.Size([98204])\n",
      "  dtype:  torch.int64\n",
      "  device:  cuda:0\n",
      "  mem:  767.289 KiB\n"
     ]
    }
   ],
   "source": [
    "utils.ten.print_tensor_info(cons_shop_item_id_counts_orig, \"cons_shop_item_id_counts_orig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([186, 356, 498, 664, 971])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_cons_img_zidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_cons_img_zzidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing counts\n",
    "\n",
    "cons_shop_item_id_counts = cons_shop_item_id_counts[desired_cons_img_zidxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons_shop_item_id_counts[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation data\n",
    "\n",
    "eval_data[\"results\"][\"train\"] = []\n",
    "\n",
    "train_desired_cons_img_zidxs = utils.arr.compute_zidxs(train_cons_img_idxs, train_desired_cons_img_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([186, 356, 498, 664, 971])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desired_cons_img_zidxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[502, 1097, 1502, 2029, 2976]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desired_cons_img_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 502, 1097, 1502, 2029, 2976])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cons_img_idxs[train_desired_cons_img_zidxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442 17413\n",
      "444 11904\n",
      "446 12964\n",
      "448 27443\n",
      "495 22872\n",
      "496 22872\n",
      "497 22872\n",
      "498 22872\n",
      "499 22872\n",
      "500 22872\n",
      "501 22872\n",
      "502 22872\n",
      "503 22872\n",
      "504 22872\n",
      "506 22872\n",
      "507 22872\n",
      "509 22872\n",
      "510 22872\n",
      "511 22872\n",
      "512 30135\n",
      "513 30135\n",
      "517 12508\n",
      "518 12508\n",
      "519 12508\n",
      "520 12508\n"
     ]
    }
   ],
   "source": [
    "idx_a, idx_b = 175, 200\n",
    "\n",
    "for train_cons_img_idx, train_cons_item_id in zip(train_cons_img_idxs[idx_a:idx_b], train_cons_item_ids[idx_a:idx_b]):\n",
    "    print(train_cons_img_idx, train_cons_item_id.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_desired_cons_img_zidxs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_img_idx = train_cons_img_idxs[train_desired_cons_img_zidxs[0]]\n",
    "cons_item_id = train_cons_item_ids[train_desired_cons_img_zidxs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(502, tensor(22872, device='cuda:0'))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cons_img_idx, cons_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'closest_shop_img_dists': [4.678122997357658,\n",
      "                            4.732720983902535,\n",
      "                            4.875929480162584,\n",
      "                            4.893084303981963,\n",
      "                            4.905858250430601,\n",
      "                            4.910721254792581,\n",
      "                            4.922969856840008,\n",
      "                            4.945568398770011,\n",
      "                            4.969878786690387,\n",
      "                            4.979967361025649],\n",
      " 'closest_shop_img_idxs': [2133,\n",
      "                           1216,\n",
      "                           106622,\n",
      "                           186259,\n",
      "                           124060,\n",
      "                           156906,\n",
      "                           1360,\n",
      "                           98885,\n",
      "                           93252,\n",
      "                           3491],\n",
      " 'closest_shop_item_ids': [9151,\n",
      "                           24290,\n",
      "                           11877,\n",
      "                           7704,\n",
      "                           10576,\n",
      "                           10718,\n",
      "                           13258,\n",
      "                           21744,\n",
      "                           26467,\n",
      "                           20749],\n",
      " 'cons_img_idx': 502,\n",
      " 'cons_item_id': 22872,\n",
      " 'num_shop_imgs': 1}\n",
      "\n",
      "  Current memory usage:\n",
      "    Device ID  0:   8.902 GiB /  11.000 GiB ( 80.93%) - NVIDIA GeForce GTX 1080 Ti\n",
      "Train split examples end\n"
     ]
    }
   ],
   "source": [
    "eval_data[\"results\"][\"train\"] = []\n",
    "\n",
    "for train_desired_cons_img_zzidx, train_desired_cons_img_zidx in enumerate(train_desired_cons_img_zidxs):\n",
    "\n",
    "    cons_img_idx = train_cons_img_idxs[train_desired_cons_img_zidx]\n",
    "    cons_item_id = train_cons_item_ids[train_desired_cons_img_zidx]\n",
    "    shop_item_id_counts = cons_shop_item_id_counts[train_desired_cons_img_zzidx]\n",
    "    closest_shop_img_idxs = shop_to_desired_cons_ordered_closest_img_idxs[train_desired_cons_img_zzidx, :]\n",
    "    closest_shop_item_ids = shop_to_desired_cons_ordered_closest_item_ids[train_desired_cons_img_zzidx, :]\n",
    "    closest_shop_img_dists = shop_to_desired_cons_ordered_closest_dists[train_desired_cons_img_zzidx, :]\n",
    "\n",
    "    mini_results_dict = {\n",
    "        \"cons_img_idx\": cons_img_idx,\n",
    "        \"cons_item_id\": cons_item_id.item(),\n",
    "        \"num_shop_imgs\": shop_item_id_counts.item(),\n",
    "        \"closest_shop_img_idxs\": closest_shop_img_idxs.tolist(),\n",
    "        \"closest_shop_item_ids\": closest_shop_item_ids.tolist(),\n",
    "        \"closest_shop_img_dists\": closest_shop_img_dists.tolist()\n",
    "    }\n",
    "\n",
    "    if train_desired_cons_img_zzidx == 0:\n",
    "        pprint.pprint(mini_results_dict)\n",
    "        print()\n",
    "\n",
    "    eval_data[\"results\"][\"train\"].append({\n",
    "        \"cons_img_idx\": cons_img_idx,\n",
    "        \"cons_item_id\": cons_item_id.item(),\n",
    "        \"num_shop_imgs\": shop_item_id_counts.item(),\n",
    "        \"closest_shop_img_idxs\": closest_shop_img_idxs.tolist(),\n",
    "        \"closest_shop_item_ids\": closest_shop_item_ids.tolist(),\n",
    "        \"closest_shop_img_dists\": closest_shop_img_dists.tolist()\n",
    "    })\n",
    "\n",
    "logger.print(\"  Current memory usage:\")\n",
    "logger.print(utils.mem.sprint_memory_usage([eval_params[\"settings\"][\"device_idx\"]], num_spaces=4))\n",
    "\n",
    "logger.print(\"Train split examples end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'closest_shop_img_dists': [4.678122997357658,\n",
      "                            4.732720983902535,\n",
      "                            4.875929480162584,\n",
      "                            4.893084303981963,\n",
      "                            4.905858250430601,\n",
      "                            4.910721254792581,\n",
      "                            4.922969856840008,\n",
      "                            4.945568398770011,\n",
      "                            4.969878786690387,\n",
      "                            4.979967361025649],\n",
      " 'closest_shop_img_idxs': [2133,\n",
      "                           1216,\n",
      "                           106622,\n",
      "                           186259,\n",
      "                           124060,\n",
      "                           156906,\n",
      "                           1360,\n",
      "                           98885,\n",
      "                           93252,\n",
      "                           3491],\n",
      " 'closest_shop_item_ids': [9151,\n",
      "                           24290,\n",
      "                           11877,\n",
      "                           7704,\n",
      "                           10576,\n",
      "                           10718,\n",
      "                           13258,\n",
      "                           21744,\n",
      "                           26467,\n",
      "                           20749],\n",
      " 'cons_img_idx': 502,\n",
      " 'cons_item_id': 22872,\n",
      " 'num_shop_imgs': 1}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(eval_data[\"results\"][\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1d9cd8eb66510c5ec86eb907d6561b8001175da1689fbe0f45c40d854d32b14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
