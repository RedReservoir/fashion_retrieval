{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import src.utils.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "layer_norm = torch.nn.LayerNorm((512), elementwise_affine=True)\n",
    "\n",
    "print(src.utils.module.get_num_params(layer_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 |   3 -  1181968\n",
      "  2 |   3 -   342256\n",
      "  4 |   3 -   109024\n",
      "  8 |   3 -    39064\n",
      " 16 |   3 -    15748\n"
     ]
    }
   ],
   "source": [
    "feature_size = (192, 28, 28)\n",
    "\n",
    "reduced_channel_ratio_list = [1, 2, 4, 8, 16]\n",
    "channel_attention_kernel_size_list = [3]\n",
    "\n",
    "for reduced_channel_ratio, channel_attention_kernel_size in itertools.product(reduced_channel_ratio_list, channel_attention_kernel_size_list):\n",
    "\n",
    "    glam_module = global_local_attention_module_pytorch.GLAM(\n",
    "        in_channels=feature_size[0],\n",
    "        feature_map_size=feature_size[1],\n",
    "        num_reduced_channels=feature_size[0] // reduced_channel_ratio,\n",
    "        kernel_size=channel_attention_kernel_size\n",
    "    )\n",
    "\n",
    "    num_params = src.utils.module.get_num_params(glam_module)\n",
    "\n",
    "    print(\"{:3d} | {:3d} - {:8d}\".format(\n",
    "        reduced_channel_ratio,\n",
    "        channel_attention_kernel_size,\n",
    "        num_params\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 |   3 -  4723216\n",
      "  2 |   3 -  1366480\n",
      "  4 |   3 -   434608\n",
      "  8 |   3 -   155296\n",
      " 16 |   3 -    62296\n"
     ]
    }
   ],
   "source": [
    "feature_size = (384, 14, 14)\n",
    "\n",
    "reduced_channel_ratio_list = [1, 2, 4, 8, 16]\n",
    "channel_attention_kernel_size_list = [3]\n",
    "\n",
    "for reduced_channel_ratio, channel_attention_kernel_size in itertools.product(reduced_channel_ratio_list, channel_attention_kernel_size_list):\n",
    "\n",
    "    glam_module = global_local_attention_module_pytorch.GLAM(\n",
    "        in_channels=feature_size[0],\n",
    "        feature_map_size=feature_size[1],\n",
    "        num_reduced_channels=feature_size[0] // reduced_channel_ratio,\n",
    "        kernel_size=channel_attention_kernel_size\n",
    "    )\n",
    "\n",
    "    num_params = src.utils.module.get_num_params(glam_module)\n",
    "\n",
    "    print(\"{:3d} | {:3d} - {:8d}\".format(\n",
    "        reduced_channel_ratio,\n",
    "        channel_attention_kernel_size,\n",
    "        num_params\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 |   3 - 18883600\n",
      "  2 |   3 -  5460880\n",
      "  4 |   3 -  1735504\n",
      "  8 |   3 -   619312\n",
      " 16 |   3 -   247840\n"
     ]
    }
   ],
   "source": [
    "feature_size = (768, 7, 7)\n",
    "\n",
    "reduced_channel_ratio_list = [1, 2, 4, 8, 16]\n",
    "channel_attention_kernel_size_list = [3]\n",
    "\n",
    "for reduced_channel_ratio, channel_attention_kernel_size in itertools.product(reduced_channel_ratio_list, channel_attention_kernel_size_list):\n",
    "\n",
    "    glam_module = global_local_attention_module_pytorch.GLAM(\n",
    "        in_channels=feature_size[0],\n",
    "        feature_map_size=feature_size[1],\n",
    "        num_reduced_channels=feature_size[0] // reduced_channel_ratio,\n",
    "        kernel_size=channel_attention_kernel_size\n",
    "    )\n",
    "\n",
    "    num_params = src.utils.module.get_num_params(glam_module)\n",
    "\n",
    "    print(\"{:3d} | {:3d} - {:8d}\".format(\n",
    "        reduced_channel_ratio,\n",
    "        channel_attention_kernel_size,\n",
    "        num_params\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2521936\n",
      "786432\n"
     ]
    }
   ],
   "source": [
    "in_feat_shape = (768, 7, 7)\n",
    "emb_size = 1024\n",
    "glam_int_channels = round(in_feat_shape[0] / 4)\n",
    "glam_1d_kernel_size = 3\n",
    "\n",
    "glam_head = src.comps.heads_glam.RetrievalHeadGLAM(\n",
    "    in_feat_shape,\n",
    "    emb_size,\n",
    "    glam_int_channels,\n",
    "    glam_1d_kernel_size\n",
    ")\n",
    "\n",
    "default_head = src.comps.heads.RetHead(\n",
    "    in_feat_shape,\n",
    "    emb_size\n",
    ")\n",
    "\n",
    "print(src.utils.module.get_num_params(glam_head))\n",
    "print(src.utils.module.get_num_params(default_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 1024, 7, 7]         786,432\n",
      " AdaptiveAvgPool2d-2           [-1, 1024, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 786,432\n",
      "Trainable params: 786,432\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 0.39\n",
      "Params size (MB): 3.00\n",
      "Estimated Total Size (MB): 3.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(default_head, input_size=in_feat_shape, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         AvgPool2d-1            [-1, 768, 1, 1]               0\n",
      "            Conv1d-2               [-1, 1, 768]               4\n",
      "LocalChannelAttention-3            [-1, 768, 7, 7]               0\n",
      "            Conv2d-4            [-1, 192, 7, 7]         147,648\n",
      "            Conv2d-5            [-1, 192, 7, 7]         331,968\n",
      "            Conv2d-6            [-1, 192, 7, 7]         331,968\n",
      "            Conv2d-7            [-1, 192, 7, 7]         331,968\n",
      "            Conv2d-8              [-1, 1, 7, 7]             769\n",
      "LocalSpatialAttention-9            [-1, 768, 7, 7]               0\n",
      "        AvgPool2d-10            [-1, 768, 1, 1]               0\n",
      "           Conv1d-11               [-1, 1, 768]               4\n",
      "           Conv1d-12               [-1, 1, 768]               4\n",
      "GlobalChannelAttention-13            [-1, 768, 7, 7]               0\n",
      "           Conv2d-14            [-1, 192, 7, 7]         147,648\n",
      "           Conv2d-15            [-1, 192, 7, 7]         147,648\n",
      "           Conv2d-16            [-1, 192, 7, 7]         147,648\n",
      "           Conv2d-17            [-1, 768, 7, 7]         148,224\n",
      "GlobalSpatialAttention-18            [-1, 768, 7, 7]               0\n",
      "             GLAM-19            [-1, 768, 7, 7]               0\n",
      "           Conv2d-20           [-1, 1024, 7, 7]         786,432\n",
      "AdaptiveAvgPool2d-21           [-1, 1024, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 2,521,933\n",
      "Trainable params: 2,521,933\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 2.65\n",
      "Params size (MB): 9.62\n",
      "Estimated Total Size (MB): 12.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(glam_head, input_size=in_feat_shape, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = src.utils.comps.create_backbone({\n",
    "    \"class\": \"ConvNeXtTinyBackbone\",\n",
    "    \"img_size\": 224\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    input_imgs = torch.rand(1, 3, 224, 224)\n",
    "    bkb_output = backbone(input_imgs)\n",
    "    glam_output = glam_head(bkb_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768, 7, 7])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bkb_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glam_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "649142fb4cdab8a2d2387ea4a1c8e262f08b2b20e4af0e114d36ea602bf8b868"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
